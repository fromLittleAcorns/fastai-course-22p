{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57e9af8-e821-4b0d-a5d3-c0aad7005f5d",
   "metadata": {},
   "source": [
    "Attention provides a means for a model to understand how parts of a model that are not necessarily closely connected in a model, can be considered as jointly linked (as per words in different parts of a sentence or one part of an image to another part in a different region)\n",
    "\n",
    "The model below is based upon attention as developed for NLP.  It is similar to the diffusers model in that at the end the inp is added to the attention, but in pytorch attention this is not the case\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7778bf2-ae12-4191-b248-80529c603d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math,torch\n",
    "from torch import nn\n",
    "from miniai.activations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbf4f770-b1a6-4642-b27e-4a186c44ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73634640-da21-43d4-b631-7d19b65a495e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.models.attention import AttentionBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e40389f9-3190-4ec1-a1e4-e61e4c656316",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "x = torch.randn(64,32,16,16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09b1af5-a510-4adb-b8d4-2a23ee1ae20e",
   "metadata": {},
   "source": [
    "Batch size 63, 32 channels, hxw = 16, 16.\n",
    "\n",
    "In the next cell the channels are moved to the end and the hw (the equivalent of sequence in NLP) are concatenated into a single vector and are usually before the channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1b0a318-9d9f-4395-b495-1d8841baa91e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = x.view(*x.shape[:2], -1).transpose(1, 2)\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71283915-d11d-42f1-84af-329acdcd4e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ni = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183da9ea-9757-473f-aeeb-a1cf11f09d46",
   "metadata": {},
   "source": [
    "Create linear layers to generate the k, q, v vectors from the input to the attention model (ni values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c88452bc-1876-4c3f-a464-47a954a6e9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sk = nn.Linear(ni, ni)\n",
    "sq = nn.Linear(ni, ni)\n",
    "sv = nn.Linear(ni, ni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df72799c-1d81-4e89-8ed6-d3f7bbad9df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = sk(t)\n",
    "q = sq(t)\n",
    "v = sv(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68d6fa91-ca52-45c2-bceb-c63f765d4c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2069f1c5-33cf-4b4c-b5a4-5844dc89f1ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([64, 32, 256]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q.shape, k.transpose(1,2).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553e0108-0090-4238-9131-09e4875f6758",
   "metadata": {},
   "source": [
    "For transformers it is common to multiply the vectors together to generate an outer product (showing effectively linkage between every pixel of either vector.\n",
    "\n",
    "Because all of the k, q, and v vectors are fed from the same x this is called self attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13e63d4b-f082-421f-abf4-66debd83e2f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d5e792-e443-4a8d-84fc-15e6e81b881c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(q@k.transpose(1,2)).softmax(dim=-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bf094ab1-3324-48d3-b9d6-1ee2bc3a953c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        # create GroupNorm (batchnorm split into a sets of channels)\n",
    "        self.norm = nn.GroupNorm(1, ni)\n",
    "        self.q = nn.Linear(ni, ni)\n",
    "        self.k = nn.Linear(ni, ni)\n",
    "        self.v = nn.Linear(ni, ni)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        # Apply normalisation\n",
    "        x = self.norm(x)\n",
    "        # Reshape to concatenate rows and to move the channels to the end\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q = self.q(x)\n",
    "        k = self.k(x)\n",
    "        v = self.v(x)\n",
    "        # Obtain the outer product of q and k.  Divide by scale to normalise\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        # Take a softmax of the above outer product across the last dimension.\n",
    "        # Then take the product of the above with the v vector. This provides a matrix\n",
    "        # of pixels x channels with a prioritization for high priority links\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        # Pass this though a final linear layer\n",
    "        x = self.proj(x)\n",
    "        # Swop the channel and hw channels, then reshape back to the original shape\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "85cc67a6-cea0-45f4-924d-d2c22cc331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sa = SelfAttention(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "02de30a6-f0d9-4c36-ab92-889a791441f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra = sa(x)\n",
    "ra.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69626d90-fdda-4749-918d-1956caf09534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2ad21f-8541-4b0d-abd4-9efd679ba6e2",
   "metadata": {},
   "source": [
    "Work out whether the attention class above provides the same output as the diffusion attention block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc034b2f-70f6-4b02-8bc6-e2c9581ee4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cp_parms(a,b):\n",
    "    b.weight = a.weight\n",
    "    b.bias = a.bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f88a414-ae2f-44be-a64b-42a18ed2b230",
   "metadata": {},
   "source": [
    "Copy the weights and biases from the above class into the diffusion attention instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe2a2294-b17b-4d09-a892-246b689e4956",
   "metadata": {},
   "outputs": [],
   "source": [
    "at = AttentionBlock(32, norm_num_groups=1)\n",
    "src = sa.q,sa.k,sa.v,sa.proj,sa.norm\n",
    "dst = at.query,at.key,at.value,at.proj_attn,at.group_norm\n",
    "for s,d in zip(src,dst): cp_parms(s,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f52c7fe4-99c1-49fb-af8c-f399c582ff68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9104,  1.4186,  0.8385, -2.1584,  0.6318, -1.2443, -0.0789, -1.6844,\n",
       "        -0.7939,  1.6117, -0.3852, -1.4307, -0.7494, -0.6010, -0.8335,  0.7477],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rb = at(x)\n",
    "rb[0,0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2276b385-07ec-4c16-888c-daf577fef372",
   "metadata": {},
   "source": [
    "It can be seen that the output of the two networks is identical given the same weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88586cc-da0e-4d9c-b6f4-c3169e86a912",
   "metadata": {},
   "source": [
    "To reduce the computation overhead the k, q, v matricies can be combine and then separated using chunks.  This makes moving them around more efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d24e903e-74df-45a7-a2c3-73a0904f725d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 96])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sqkv = nn.Linear(ni, ni*3)\n",
    "st = sqkv(t)\n",
    "st.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "926bcb71-b4ff-4778-9af7-3c19642c1404",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 32])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q,k,v = torch.chunk(st, 3, dim=-1)\n",
    "q.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2593f765-613d-412f-8e70-463cfd191e78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 256])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(k@q.transpose(1,2)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e9a543b0-669a-4fff-853c-14fc90649c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni):\n",
    "        super().__init__()\n",
    "        self.scale = math.sqrt(ni)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        inp = x\n",
    "        n,c,h,w = x.shape\n",
    "        # Apply normalisation\n",
    "        x = self.norm(x)\n",
    "        # Reshape to concatenate rows and to move the channels to the end\n",
    "        x = x.view(n, c, -1).transpose(1, 2)\n",
    "        q, k, v = torch.chunk(self.qkv(x), 3, dim=-1)\n",
    "        # Obtain the outer product of q and k.  Divide by scale to normalise\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        # Take a softmax of the above outer product across the last dimension.\n",
    "        # Then take the product of the above with the v vector. This provides a matrix\n",
    "        # of pixels x channels with a prioritization for high priority links\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        # Pass this though a final linear layer\n",
    "        x = self.proj(x)\n",
    "        # Swop the channel and hw channels, then reshape back to the original shape\n",
    "        x = x.transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a109979c-b3b1-4e54-a44b-eceeed7c0d7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttention(32)\n",
    "sa(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b0f6b08-79e4-48c9-85bb-f0b6c4285679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0085, grad_fn=<StdBackward0>)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa(x).std()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42ed87e-38b8-41dc-875d-6795ec67095d",
   "metadata": {},
   "source": [
    "### Multi-Head Self Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf2bb56-9068-4323-aeea-aae24ecf17af",
   "metadata": {},
   "source": [
    "To allow the model to allow multiple things to be focussed upon we can provide multiple heads.  \n",
    "This is done by splitting the channels by the number of heads, and then processing each head independently. To \n",
    "do so the channels are taken out and made into independent items with the batch, hence \n",
    "each set of channels and image appears as a separate sub image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e062dbd-ec0d-45cd-8744-e4e4c44a33ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heads_to_batch(x, heads):\n",
    "    \"\"\" Initially splits the channels into to dimensions (heads, channels/heads)\n",
    "    Then transposes sl and heads. Then reshapes so that each combination of head and batch is separate at the first dim\n",
    "    \"\"\"\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(n, sl, heads, -1)\n",
    "    return x.transpose(2, 1).reshape(n*heads,sl,-1)\n",
    "\n",
    "def batch_to_heads(x, heads):\n",
    "    n,sl,d = x.shape\n",
    "    x = x.reshape(-1, heads, sl, d)\n",
    "    return x.transpose(2, 1).reshape(-1,sl,d*heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "da78196c-4f20-4dc0-9808-5d5c2ad1c237",
   "metadata": {},
   "outputs": [],
   "source": [
    "from einops import rearrange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03e02aa6-0c86-43ed-97c7-0ae5aaeb4087",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 256, 32]), torch.Size([512, 256, 4]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2 = rearrange(t , 'n s (h d) -> (n h) s d', h=8)\n",
    "t.shape, t2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c585f314-67a3-4053-859d-712bbeb662aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "t3 = rearrange(t2, '(n h) s d -> n s (h d)', h=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ccc6d92e-1b9f-4f89-b3aa-cc2293c5b132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([512, 256, 4]), torch.Size([64, 256, 32]))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2.shape,t3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "795c7672-636e-44d2-bbf7-ce864d3842fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(t==t3).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b5ed0caa-7955-49c1-b3ed-f70fc98e3095",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttentionMultiHead(nn.Module):\n",
    "    def __init__(self, ni, nheads):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.scale = math.sqrt(ni/nheads)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "    \n",
    "    def forward(self, inp):\n",
    "        n,c,h,w = inp.shape\n",
    "        x = self.norm(inp).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.qkv(x)\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x).transpose(1,2).reshape(n,c,h,w)\n",
    "        return x+inp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c403755f-9872-4654-af0a-e478702bcaec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 32, 16, 16])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sa = SelfAttentionMultiHead(32, 4)\n",
    "sx = sa(x)\n",
    "sx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "033b1c6f-3a88-4ce6-a097-b80b8996de13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0222, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0069, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sx.mean(),sx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7677fc6b-8e7d-434f-b92a-e75a5c8a1843",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53bee61c-4f1c-4878-b885-5397931e72d1",
   "metadata": {},
   "source": [
    "### Note for Pytorch multihead attention.\n",
    "\n",
    "By default pytorch MHA will be default not expect batch first unless the flag below is set.  The dafault is for the batch to be second.\n",
    "\n",
    "Also, it is necessary to pass in the same to each input (kqv) in order to make it self attention.  If different things are passed in then it become cross attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9abda466-28e7-4c0e-99a0-2bc7cd1a5244",
   "metadata": {},
   "outputs": [],
   "source": [
    "nm = nn.MultiheadAttention(32, num_heads=8, batch_first=True)\n",
    "nmx,nmw = nm(t,t,t)\n",
    "nmx = nmx+t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f3a6aab6-5c1d-4b66-b05c-82ed605a51d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-0.0015, grad_fn=<MeanBackward0>),\n",
       " tensor(1.0034, grad_fn=<StdBackward0>))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmx.mean(),nmx.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6e6c4590-2501-4027-8d3f-8d0fb13492a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4115, 0.3150, 0.3019, 0.0508, 0.6761, 0.8469],\n",
       "        [0.7011, 0.2775, 0.5324, 0.3479, 0.7456, 0.9074],\n",
       "        [0.4694, 0.9891, 0.9687, 0.6516, 0.6563, 0.5602],\n",
       "        [0.0490, 0.9218, 0.8198, 0.7353, 0.5030, 0.9022],\n",
       "        [0.1250, 0.4525, 0.6666, 0.2004, 0.3990, 0.2803],\n",
       "        [0.3316, 0.7570, 0.0450, 0.0627, 0.5231, 0.9098]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.rand([6,6])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cecb6f55-81fe-4ae6-afb6-8980846a3209",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0684e-02, 4.0694e-03, 3.5706e-03, 2.8967e-04, 1.5057e-01, 8.3082e-01],\n",
       "        [9.3837e-02, 1.3574e-03, 1.7362e-02, 2.7434e-03, 1.4642e-01, 7.3828e-01],\n",
       "        [2.9040e-03, 5.2495e-01, 4.2817e-01, 1.7955e-02, 1.8823e-02, 7.2026e-03],\n",
       "        [6.8814e-05, 4.2497e-01, 1.5320e-01, 6.5852e-02, 6.4489e-03, 3.4946e-01],\n",
       "        [3.6371e-03, 9.6244e-02, 8.1883e-01, 7.7308e-03, 5.6367e-02, 1.7192e-02],\n",
       "        [2.4837e-03, 1.7486e-01, 1.4142e-04, 1.6868e-04, 1.6858e-02, 8.0548e-01]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(test/0.1, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1fe40332-7f23-482e-9ee3-f1434b04f3d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1575, 0.1430, 0.1411, 0.1098, 0.2052, 0.2434],\n",
       "        [0.1826, 0.1195, 0.1543, 0.1283, 0.1909, 0.2244],\n",
       "        [0.1277, 0.2148, 0.2104, 0.1532, 0.1540, 0.1399],\n",
       "        [0.0872, 0.2088, 0.1886, 0.1733, 0.1374, 0.2048],\n",
       "        [0.1304, 0.1810, 0.2242, 0.1406, 0.1715, 0.1523],\n",
       "        [0.1421, 0.2174, 0.1067, 0.1086, 0.1720, 0.2533]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(test, dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a82df6-78c2-45f7-8d27-70074139c050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c22penv",
   "language": "python",
   "name": "c22penv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
