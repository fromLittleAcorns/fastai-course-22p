{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd730a43-1425-496e-8ab7-772ef67adf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617c83a2-aced-44d2-b592-c78d41ea8c28",
   "metadata": {},
   "source": [
    "## Diffusion Unet with Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bafd7f8-8234-4617-a4f0-44f529245c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4fced49-5ca2-4d00-a416-781fdfc47bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from miniai.imports import *\n",
    "\n",
    "from einops import rearrange\n",
    "from fastprogress import progress_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d056382c-1a95-4737-8ab8-f9e053e94a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray_r'\n",
    "mpl.rcParams['figure.dpi'] = 70\n",
    "\n",
    "import logging\n",
    "logging.disable(logging.WARNING)\n",
    "\n",
    "set_seed(42)\n",
    "if fc.defaults.cpus>7: fc.defaults.cpus=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6d8a1ca5-306e-4274-b568-f5af704a2374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7594eca2e35c4053af9d6ba0ad5fa82a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xl,yl = 'image','label'\n",
    "name = \"fashion_mnist\"\n",
    "bs = 512\n",
    "dsd = load_dataset(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff8e8ec-d5a6-4f56-9553-6b635f707c37",
   "metadata": {},
   "source": [
    "This notebook has reverted to the cosine scheduling from the Karras one (notebook 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c37b131d-3090-4b76-8d52-9c20aaf1703d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def abar(t):\n",
    "    \"\"\" Generate alpha bar based upon the cosine schedule\n",
    "    Will equal 1.0 at t=0 and 0 at t=1\n",
    "    \"\"\"\n",
    "    return (t*math.pi/2).cos()**2\n",
    "\n",
    "def inv_abar(x): return x.sqrt().acos()*2/math.pi\n",
    "\n",
    "def noisify(x0):\n",
    "    \"\"\" Add noise to images based upon a randomly chosen timestep (between 0 and 1)\n",
    "    using the aphabar function and cosine scheduling\n",
    "    \n",
    "    parameters\n",
    "        x0: A batch of flattened images\n",
    "        \n",
    "    returns\n",
    "        A tuple of noisified images, the time step matrix used to generate the noised images and\n",
    "        \n",
    "    \"\"\"\n",
    "    device = x0.device\n",
    "    n = len(x0)\n",
    "    # Note that in the line below it seems that to much be picking up the device and type from x0. Check\n",
    "    # this when using a gpu\n",
    "    # Generate an image of random noise\n",
    "    t = torch.rand(n,).to(x0).clamp(0,0.999)\n",
    "    # Generate an image of random noise\n",
    "    ε = torch.randn(x0.shape, device=device)\n",
    "    # Create matrix to allow broadcasting of the alphabar over the batch of data\n",
    "    abar_t = abar(t).reshape(-1, 1, 1, 1).to(device)\n",
    "    xt = abar_t.sqrt()*x0 + (1-abar_t).sqrt()*ε\n",
    "    return (xt, t.to(device)), ε\n",
    "\n",
    "def collate_ddpm(b): return noisify(default_collate(b)[xl])\n",
    "def dl_ddpm(ds): return DataLoader(ds, batch_size=bs, collate_fn=collate_ddpm, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1040338d-db14-4365-ab73-396715c4464c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 50])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = torch.rand(16, 50)\n",
    "x0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65561c29-9e25-40b0-a48c-b9298edf1ffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6d7e90-bf9b-4b92-a6b7-e10ad4f80ee7",
   "metadata": {},
   "source": [
    "This notebook is using -0.5 in the scaling part.  This is instead of x2-1.  the latter will give mean zero standard deviation of one, the former will have mean zero but half the standatd deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee1cc4a6-14ed-4f77-be71-b1e8e4cd3498",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/c22penv/lib/python3.9/site-packages/torch/utils/data/dataloader.py:554: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    }
   ],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9ab77d-a56e-4048-82ec-96849d0c199d",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b00ce06-8f84-46c8-aeda-5ebdfeebc21e",
   "metadata": {},
   "source": [
    "This is based upon the diffuser model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bfb02f06-8de4-4873-8ed8-7622247c390f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def timestep_embedding(tsteps, emb_dim, max_period= 10000):\n",
    "    \"\"\"\n",
    "    args:\n",
    "        tsteps (int): number of time steps\n",
    "        emb_dim (int): embedding dimension length\n",
    "        max_period (int): The duration of the longest cycle.  Smaller values will increase the \n",
    "            frequency of the embeddings\n",
    "    returns:\n",
    "    The embedding matrix\n",
    "        \n",
    "    \"\"\"\n",
    "    exponent = -math.log(max_period) * torch.linspace(0, 1, emb_dim//2, device=tsteps.device)\n",
    "    emb = tsteps[:,None].float() * exponent.exp()[None,:]\n",
    "    emb = torch.cat([emb.sin(), emb.cos()], dim=-1)\n",
    "    return F.pad(emb, (0,1,0,0)) if emb_dim%2==1 else emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a31748e-3f09-4365-96b1-3e52ca35c2c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bd5c3e1-3cc9-4b33-abd1-ec8442e25993",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def pre_conv(ni, nf, ks=3, stride=1, act=nn.SiLU, norm=None, bias=True):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Conv2d(ni, nf, stride=stride, kernel_size=ks, padding=ks//2, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6939561f-e1d0-4f1d-a558-0ca84552fb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def upsample(nf): return nn.Sequential(nn.Upsample(scale_factor=2.), nn.Conv2d(nf, nf, 3, padding=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfbd75a8-9a89-4f83-a984-e2a5c0170fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lin(ni, nf, act=nn.SiLU, norm=None, bias=True):\n",
    "    layers = nn.Sequential()\n",
    "    if norm: layers.append(norm(ni))\n",
    "    if act : layers.append(act())\n",
    "    layers.append(nn.Linear(ni, nf, bias=bias))\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6c4ab8d-48c9-49ab-9518-7d14af4a36c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This version is giving poor results - use the cell below instead\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(ni, ni//attn_chans, batch_first=True)\n",
    "        self.norm = nn.BatchNorm2d(ni)\n",
    "\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        x = self.norm(x).view(n, c, -1).transpose(1, 2)\n",
    "        x = self.attn(x, x, x, need_weights=False)[0]\n",
    "        return x.transpose(1,2).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d1d0b3-8ba7-4b03-9a17-5ce7e9957eef",
   "metadata": {},
   "source": [
    "The cell below does not use the native pytorch self attention.  In this case we are not passing the input  through to the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc004d4c-e2d4-46ac-bad3-ac69e321d5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, ni, attn_chans, transpose=True):\n",
    "        super().__init__()\n",
    "        self.nheads = ni//attn_chans\n",
    "        self.scale = math.sqrt(ni/self.nheads)\n",
    "        self.norm = nn.LayerNorm(ni)\n",
    "        self.qkv = nn.Linear(ni, ni*3)\n",
    "        self.proj = nn.Linear(ni, ni)\n",
    "        self.t = transpose\n",
    "    \n",
    "    def forward(self, x):\n",
    "        n,c,s = x.shape\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        x = self.norm(x)\n",
    "        x = self.qkv(x)\n",
    "        # d is the channels per head, s is the sequence length\n",
    "        x = rearrange(x, 'n s (h d) -> (n h) s d', h=self.nheads)\n",
    "        q,k,v = torch.chunk(x, 3, dim=-1)\n",
    "        s = (q@k.transpose(1,2))/self.scale\n",
    "        x = s.softmax(dim=-1)@v\n",
    "        x = rearrange(x, '(n h) s d -> n s (h d)', h=self.nheads)\n",
    "        x = self.proj(x)\n",
    "        if self.t: x = x.transpose(1, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fadf733-5e36-4f39-84ce-6b7b9f5b4b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SelfAttention2D(SelfAttention):\n",
    "    \"\"\" Simply unwraps the image, passes it through the attention network and then reshapes it\n",
    "    \"\"\"\n",
    "    def forward(self, x):\n",
    "        n,c,h,w = x.shape\n",
    "        return super().forward(x.view(n, c, -1)).reshape(n,c,h,w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0467e2af-1317-416a-b48d-158e43b75d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbResBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf=None, ks=3, act=nn.SiLU, norm=nn.BatchNorm2d, attn_chans=0):\n",
    "        super().__init__()\n",
    "        if nf is None: nf = ni\n",
    "        self.emb_proj = nn.Linear(n_emb, nf*2)\n",
    "        self.conv1 = pre_conv(ni, nf, ks, act=act, norm=norm)\n",
    "        self.conv2 = pre_conv(nf, nf, ks, act=act, norm=norm)\n",
    "        self.idconv = fc.noop if ni==nf else nn.Conv2d(ni, nf, 1)\n",
    "        self.attn = False\n",
    "        if attn_chans: self.attn = SelfAttention2D(nf, attn_chans)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        inp = x\n",
    "        x = self.conv1(x)\n",
    "        emb = self.emb_proj(F.silu(t))[:, :, None, None]\n",
    "        scale,shift = torch.chunk(emb, 2, dim=1)\n",
    "        x = x*(1+scale) + shift\n",
    "        x = self.conv2(x)\n",
    "        x = x + self.idconv(inp)\n",
    "        if self.attn: x = x + self.attn(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf583dd4-b56c-40eb-a9f5-ec105c5a7d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def saved(m, blk):\n",
    "    \"\"\"Creates a function that will save the values of the embedding layers to facilitate passing to the \n",
    "    decoding part\n",
    "    \"\"\"\n",
    "    m_ = m.forward\n",
    "\n",
    "    @wraps(m.forward)\n",
    "    def _f(*args, **kwargs):\n",
    "        res = m_(*args, **kwargs)\n",
    "        blk.saved.append(res)\n",
    "        return res\n",
    "\n",
    "    m.forward = _f\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ae96acbd-4e7a-4ec6-b796-d21e0119ccd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, nf, add_down=True, num_layers=1, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList([saved(EmbResBlock(n_emb, ni if i==0 else nf, nf, attn_chans=attn_chans), self)\n",
    "                                      for i in range(num_layers)])\n",
    "        self.down = saved(nn.Conv2d(nf, nf, 3, stride=2, padding=1), self) if add_down else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        self.saved = []\n",
    "        for resnet in self.resnets: x = resnet(x, t)\n",
    "        x = self.down(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cfd7f316-d994-4b8b-8fa3-a2cf5b050ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self, n_emb, ni, prev_nf, nf, add_up=True, num_layers=2, attn_chans=0):\n",
    "        super().__init__()\n",
    "        self.resnets = nn.ModuleList(\n",
    "            [EmbResBlock(n_emb, (prev_nf if i==0 else nf)+(ni if (i==num_layers-1) else nf), nf, attn_chans=attn_chans)\n",
    "            for i in range(num_layers)])\n",
    "        self.up = upsample(nf) if add_up else nn.Identity()\n",
    "\n",
    "    def forward(self, x, t, ups):\n",
    "        for resnet in self.resnets: x = resnet(torch.cat([x, ups.pop()], dim=1), t)\n",
    "        return self.up(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7dda3-2779-4dbc-9656-78876f790e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EmbUNetModel(nn.Module):\n",
    "    def __init__( self, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1, attn_chans=8, attn_start=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        n = len(nfs)\n",
    "        for i in range(n):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=n-1, num_layers=num_layers,\n",
    "                                        attn_chans=0 if i=n-attn_start else attn_chans))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        emb = self.emb_mlp(temb)\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b52ee6-8336-4bf3-b519-4b3ca53d1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = EmbUNetModel(in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c01bf11-ea91-43ec-8158-7dd6dc788fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "703f7665-78b9-42b7-b7cd-b1488793dbbe",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "34720ff2-75c5-4bbc-ae87-7a0e15444aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from miniai.fid import ImageEval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6cf8c7-6929-475e-969e-18a661415477",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmodel = torch.load('/home/models/data_aug2.pkl')\n",
    "del(cmodel[8])\n",
    "del(cmodel[7])\n",
    "\n",
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))*2-1 for o in b[xl]]\n",
    "\n",
    "bs = 2048\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders.from_dd(tds, bs, num_workers=fc.defaults.cpus)\n",
    "\n",
    "dt = dls.train\n",
    "xb,yb = next(iter(dt))\n",
    "\n",
    "ie = ImageEval(cmodel, dls, cbs=[DeviceCB()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033f975a-84b5-4639-b8c7-3cb522c532b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (2048,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933756bc-90c5-47e9-9da9-24e7b3b99998",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def ddim_step(x_t, noise, abar_t, abar_t1, bbar_t, bbar_t1, eta, sig, clamp=True):\n",
    "    sig = ((bbar_t1/bbar_t).sqrt() * (1-abar_t/abar_t1).sqrt()) * eta\n",
    "    x_0_hat = ((x_t-(1-abar_t).sqrt()*noise) / abar_t.sqrt())\n",
    "    if clamp: x_0_hat = x_0_hat.clamp(-1,1)\n",
    "    if bbar_t1<=sig**2+0.01: sig=0.  # set to zero if very small or NaN\n",
    "    x_t = abar_t1.sqrt()*x_0_hat + (bbar_t1-sig**2).sqrt()*noise\n",
    "    x_t += sig * torch.randn(x_t.shape).to(x_t)\n",
    "    return x_0_hat,x_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94720bc-9096-47a6-8b87-d03fde9e42b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def sample(f, model, sz, steps, eta=1., clamp=True):\n",
    "    model.eval()\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100), clamp=clamp)\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20ecc58-1561-4ea3-b2fa-7266be7674fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(42)\n",
    "preds = sample(ddim_step, model, sz, steps=100, eta=1.)\n",
    "s = (preds[-1]*2)\n",
    "s.min(),s.max(),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ae6fd3-fce5-4e26-b3e3-2672d09d3b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(s[:25].clamp(-1,1), imsize=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c7b3c-1434-4727-84da-af6e51900cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ie.fid(s),ie.kid(s),s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028e914-5f9f-4155-96c8-67086e2cd88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=100, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec581910-918f-4b38-9462-d83550cfa75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=100, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1feb4-ece9-4cb9-a618-66bbb550cec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = sample(ddim_step, model, sz, steps=50, eta=1.)\n",
    "ie.fid(preds[-1]*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e42dd0-483d-4ae9-a931-5e73c5aca8d5",
   "metadata": {},
   "source": [
    "### Conditional model\n",
    "Train the model to predict specific items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79facd25-5ac3-4fff-8835-235af8d5522d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_ddpm(b):\n",
    "    b = default_collate(b)\n",
    "    (xt,t),eps = noisify(b[xl])\n",
    "    return (xt,t,b[yl]),eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a41ac2-c720-4ba7-a4f4-72d0c83d3deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@inplace\n",
    "def transformi(b): b[xl] = [F.pad(TF.to_tensor(o), (2,2,2,2))-0.5 for o in b[xl]]\n",
    "\n",
    "tds = dsd.with_transform(transformi)\n",
    "dls = DataLoaders(dl_ddpm(tds['train']), dl_ddpm(tds['test']))\n",
    "\n",
    "dl = dls.train\n",
    "(xt,t,c),eps = b = next(iter(dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640cb1c3-d2e5-4861-9a2c-d9206c2afe82",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondUNetModel(nn.Module):\n",
    "    \"\"\" The difference with the conditional model is that we pass in the class that is required\n",
    "    and then create class embeddings.  These are added to the timestep embeddings and hence are\n",
    "    available to the model\n",
    "    \"\"\"\n",
    "    def __init__( self, n_classes, in_channels=3, out_channels=3, nfs=(224,448,672,896), num_layers=1):\n",
    "        super().__init__()\n",
    "        self.conv_in = nn.Conv2d(in_channels, nfs[0], kernel_size=3, padding=1)\n",
    "        self.n_temb = nf = nfs[0]\n",
    "        n_emb = nf*4\n",
    "        self.cond_emb = nn.Embedding(n_classes, n_emb)\n",
    "        self.emb_mlp = nn.Sequential(lin(self.n_temb, n_emb, norm=nn.BatchNorm1d),\n",
    "                                     lin(n_emb, n_emb))\n",
    "        self.downs = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            ni = nf\n",
    "            nf = nfs[i]\n",
    "            self.downs.append(DownBlock(n_emb, ni, nf, add_down=i!=len(nfs)-1, num_layers=num_layers))\n",
    "        self.mid_block = EmbResBlock(n_emb, nfs[-1])\n",
    "\n",
    "        rev_nfs = list(reversed(nfs))\n",
    "        nf = rev_nfs[0]\n",
    "        self.ups = nn.ModuleList()\n",
    "        for i in range(len(nfs)):\n",
    "            prev_nf = nf\n",
    "            nf = rev_nfs[i]\n",
    "            ni = rev_nfs[min(i+1, len(nfs)-1)]\n",
    "            self.ups.append(UpBlock(n_emb, ni, prev_nf, nf, add_up=i!=len(nfs)-1, num_layers=num_layers+1))\n",
    "        self.conv_out = pre_conv(nfs[0], out_channels, act=nn.SiLU, norm=nn.BatchNorm2d, bias=False)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        x,t,c = inp\n",
    "        temb = timestep_embedding(t, self.n_temb)\n",
    "        cemb = self.cond_emb(c)\n",
    "        emb = self.emb_mlp(temb) + cemb\n",
    "        x = self.conv_in(x)\n",
    "        saved = [x]\n",
    "        for block in self.downs: x = block(x, emb)\n",
    "        saved += [p for o in self.downs for p in o.saved]\n",
    "        x = self.mid_block(x, emb)\n",
    "        for block in self.ups: x = block(x, emb, saved)\n",
    "        return self.conv_out(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a74cdf1a-b7b2-479e-9692-13c2ddeb3f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-2\n",
    "epochs = 25\n",
    "opt_func = partial(optim.Adam, eps=1e-5)\n",
    "tmax = epochs * len(dls.train)\n",
    "sched = partial(lr_scheduler.OneCycleLR, max_lr=lr, total_steps=tmax)\n",
    "cbs = [DeviceCB(), ProgressCB(plot=True), MetricsCB(), BatchSchedCB(sched), MixedPrecision()]\n",
    "model = CondUNetModel(10, in_channels=1, out_channels=1, nfs=(32,64,128,256), num_layers=2)\n",
    "learn = Learner(model, dls, nn.MSELoss(), lr=lr, cbs=cbs, opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a94ba7e-a5eb-4b67-a950-09721c1105d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.fit(epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f155d71d-43ff-479d-8aba-bb7188e61706",
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = (256,1,32,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3eb55e-523e-4825-9d92-470da62085af",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad()\n",
    "def cond_sample(c, f, model, sz, steps, eta=1.):\n",
    "    ts = torch.linspace(1-1/steps,0,steps)\n",
    "    x_t = torch.randn(sz).cuda()\n",
    "    c = x_t.new_full((sz[0],), c, dtype=torch.int32)\n",
    "    preds = []\n",
    "    for i,t in enumerate(progress_bar(ts)):\n",
    "        t = t[None].cuda()\n",
    "        abar_t = abar(t)\n",
    "        noise = model((x_t, t, c))\n",
    "        abar_t1 = abar(t-1/steps) if t>=1/steps else torch.tensor(1)\n",
    "        x_0_hat,x_t = f(x_t, noise, abar_t, abar_t1, 1-abar_t, 1-abar_t1, eta, 1-((i+1)/100))\n",
    "        preds.append(x_0_hat.float().cpu())\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461987e8-3464-480d-9f52-27f2cc653164",
   "metadata": {},
   "outputs": [],
   "source": [
    "lbls = dsd['train'].features[yl].names\n",
    "lbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd50bb79-520a-486c-adc4-ddb84126bf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "preds = sample(cid, ddim_step, model, sz, steps=100, eta=1.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41df9124-6235-4295-ac58-731e5ffdb6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "cid = 0\n",
    "preds = sample(cid, ddim_step, model, sz, steps=100, eta=0.)\n",
    "s = (preds[-1]*2)\n",
    "show_images(s[:25].clamp(-1,1), imsize=1.5, suptitle=lbls[cid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b676fbd-5a7a-4af0-aca9-c73a4cc2855a",
   "metadata": {},
   "source": [
    "### Export "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e58abf-1b7f-4496-a67e-80ac2201bff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "c22penv",
   "language": "python",
   "name": "c22penv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
