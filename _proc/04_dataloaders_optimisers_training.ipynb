{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ecc24f7b",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Notebook to establish mini-batch training from first principles\"\n",
    "author: \"John Richmond\"\n",
    "format:\n",
    "  html:\n",
    "    code-fold: False\n",
    "  pdf:\n",
    "    geometry:\n",
    "      - top=30mm\n",
    "      - left=20mm\n",
    "jupyter: python3\n",
    "toc: true\n",
    "number-sections: False\n",
    "shift-heading-level-by: -1\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f170c962",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Notebook based upon the fastai course 22p, \"Part 2 of Practical Deep Learning for Coders\".  This notebook builds the capability for training a model in batches.  As such it covers:\n",
    "* understanding managing and accessing model parameters\n",
    "* Cross entropy loss for classification\n",
    "* dataloaders, including samplers and multiprocessing\n",
    "* optimisers - implementation of SGD and how to implement using model parameters\n",
    "* setting up training loops with validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a1d513",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from fastcore.test import test_close\n",
    "\n",
    "torch.set_printoptions(precision=2, linewidth=140, sci_mode=False)\n",
    "torch.manual_seed(1)\n",
    "mpl.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "path_to_data = Path('/Users/johnrichmond/local_datasets') / Path('data')\n",
    "path_to_data.mkdir(exist_ok=True)\n",
    "path_gz = path_to_data / 'mnist.pkg.gz'\n",
    "\n",
    "with gzip.open(path_gz, 'rb') as f: ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding='latin-1')\n",
    "x_train, y_train, x_valid, y_valid = map(tensor, [x_train, y_train, x_valid, y_valid])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bcd2ec3",
   "metadata": {},
   "source": [
    "## Initial setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "670f5a7f",
   "metadata": {},
   "source": [
    "### Data\n",
    "As before use Mnist data with a single hidden layer of 50 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c62f40df",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 784, tensor(10))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n,m = x_train.shape\n",
    "c = y_train.max()+1\n",
    "nh = 50\n",
    "n, m, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adc078",
   "metadata": {},
   "source": [
    "Create a simple model that inherits from the pytorch nn.Module class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38965869",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.layers = [nn.Linear(n_in,nh), nn.ReLU(), nn.Linear(nh,n_out)]\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdfbd2ac",
   "metadata": {},
   "source": [
    "Pass the training data though the model and store the predictions.  Check the shape of the output and also what the range of the preds is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf5e915e",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000, 10]),\n",
       " tensor(-0.35, grad_fn=<MinBackward1>),\n",
       " tensor(0.42, grad_fn=<MaxBackward1>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(n_in=m, nh=nh, n_out=c)\n",
    "preds = model(x_train.clone())\n",
    "preds.shape, preds.min(), preds.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb15412",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "In the previous notebook a very simple loss function was used.  This will now be replaced with a cross entropy loss.  There are several \"tricks\" that are used to take what is basically a relatively simple concept and implement it in a robust and efficient fashion.  These will be explained.\n",
    "\n",
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x{_i}}}{e^{x{_1}}+e^{x{_2}} + \\cdots + e^{x{_n}}}$$\n",
    "\n",
    "Or:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10bcf93",
   "metadata": {},
   "source": [
    "$$\\hbox{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum\\limits_{0 \\leq j < n}{e^{x_{j}}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "6fe21bf5",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Note that this relies upon broadcasting the sum along axis 1 to make the divide work\n",
    "def log_softmax(x): return (x.exp() / (x.exp().sum(axis=-1, keepdim=True))).log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "968fcf08",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<LogBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edbd7fe",
   "metadata": {},
   "source": [
    "Since the log of a division is the same as subtracting the logs of each value this can be further simplified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f2ba659",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x): return (x - (x.exp().sum(axis=-1, keepdim=True)).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23d8c6ef",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_softmax(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272139b1",
   "metadata": {},
   "source": [
    "Then, there is a way to compute the log of the sum of exponentials in a more stable way, called the [LogSumExp trick](https://en.wikipedia.org/wiki/LogSumExp). The idea is to use the following formula:\n",
    "\n",
    "$$\\log \\left ( \\sum_{j=1}^{n} e^{x_{j}} \\right ) = \\log \\left ( e^{a} \\sum_{j=1}^{n} e^{x_{j}-a} \\right ) = a + \\log \\left ( \\sum_{j=1}^{n} e^{x_{j}-a} \\right )$$\n",
    "\n",
    "Where a is the maximum of the $x_j$ values, and is used to scale other values, preventing numerical overflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e253e126",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = preds.clone()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47ee7da9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def logsumexp(x):\n",
    "    # obtain the maximum x in each input row\n",
    "    max_row_vals = x.max(axis=-1)[0]\n",
    "    return (max_row_vals + (x - max_row_vals[:, None]).exp().sum(-1).log())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "933d4927",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2.28, 2.30, 2.29,  ..., 2.30, 2.28, 2.30], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logsumexp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75636283",
   "metadata": {},
   "source": [
    "Pytorch already has a pre build logsumexp and this can be used instead of the above.  To check the same values are returned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ac7e00",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "test_close(logsumexp(x), x.logsumexp(dim=-1, keepdim=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59b39881",
   "metadata": {},
   "source": [
    "Hence it is possible to simplify the above log softmax to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8373f58e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def log_softmax(x):\n",
    "    return x-x.logsumexp(-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1cea63cb",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.37, -2.49, -2.36,  ..., -2.31, -2.28, -2.22],\n",
       "        [-2.37, -2.44, -2.44,  ..., -2.27, -2.26, -2.16],\n",
       "        [-2.48, -2.33, -2.28,  ..., -2.30, -2.30, -2.27],\n",
       "        ...,\n",
       "        [-2.33, -2.52, -2.34,  ..., -2.31, -2.21, -2.16],\n",
       "        [-2.38, -2.38, -2.33,  ..., -2.29, -2.26, -2.17],\n",
       "        [-2.33, -2.55, -2.36,  ..., -2.29, -2.27, -2.16]], grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsm_preds = log_softmax(x)\n",
    "lsm_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23934adc",
   "metadata": {},
   "source": [
    "Having calculated the log softmax we now need a proper loss function.  For classification we can use the cross entropy loss.  This is defined as:\n",
    "\n",
    "$$ CE Loss = \\sum_i{x_i \\log{ p(x_i)}}$$\n",
    "\n",
    "where x is a vector of targets.  Note that in Pytorch this is not one hot encoded and so the value can be used as an index to select the column of the preds that it refers to. \n",
    "\n",
    "In this case the softmax can be interpreted as a probability (since it adds up to one for each case) and the output from the log_softmax is $log(p(x_i))$ is a vector for each class.\n",
    "\n",
    "The CE loss can therefore be obtained by taking the mean of the CE loss over the dataset.  Since x is zero over every element apart from the target, where it is one, this in practice means that all that has to be done is to calculate $-\\log{p(x_i)}$ where i is the index of the actual target\n",
    "\n",
    "Obtaining the index of the target can be done using the target as an index (as discussed above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6aaba4a5",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([50000]), tensor([5, 0, 4]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, y_train[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce61d7c",
   "metadata": {},
   "source": [
    "Since y_train is a vector we can use it to select the column as below.  This could be summed and divided by the number of datapoint but the alternative is used, which is to take the mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d26a4b1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def nll(preds, targets):\n",
    "    return -preds[range(preds.shape[0]), targets].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "22775ad2",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = nll(lsm_preds, y_train)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897f13d4",
   "metadata": {},
   "source": [
    "Check that the loss value calculate above matches that from Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "485baa44",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "test_close(F.nll_loss(F.log_softmax(x, dim=1), y_train), loss, 1.e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "868dcacb",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "test_close(F.cross_entropy(x, y_train), loss, 1.e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea623ca",
   "metadata": {},
   "source": [
    "We have now generate a loss function from first principles and can use Pytorch's version moving forward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98f4da1",
   "metadata": {},
   "source": [
    "## Development of a training loop\n",
    "\n",
    "The training loop repeats the following iteratively:\n",
    "* Load a batch of input data and the corresponding target outputs\n",
    "* Calculate the predictions from the input data\n",
    "* Calculate the loss based upon the targets and predictions\n",
    "* Calculate the gradients with respect to the loss of all of the model parameters\n",
    "* Update the parameters based upon the gradients so as to reduce the loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d89406",
   "metadata": {},
   "source": [
    "### A simple training loop\n",
    "Before we train it is convenient to have a way to ascertain the accuracy of the model. To develop this a single batch of data will be processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b8f9c0e",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 10]),\n",
       " tensor([-0.09, -0.21, -0.08,  0.10, -0.04,  0.08, -0.04, -0.03,  0.01,  0.06], grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bs = 64\n",
    "# load a minibatch\n",
    "xb = x_train[0:bs]\n",
    "yb = y_train[0:bs]\n",
    "preds = model(xb)\n",
    "preds.shape, preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2740fa57",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.30, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a loss function and calculate the loss for the batch\n",
    "loss_func = F.cross_entropy\n",
    "loss = loss_func(preds, yb)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "837428f4",
   "metadata": {},
   "source": [
    "#### Calculate Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6affb0c3",
   "metadata": {},
   "source": [
    "The predicted class can be determined from the index of the class having the highest predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d9305dfd",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 9, 3, 8, 5, 9, 3, 9, 3, 9, 5, 3, 9, 9, 3, 9, 9, 5, 8, 7, 9, 5, 3, 8, 9, 5, 9, 5, 5, 9, 3, 5, 9, 7, 5, 7, 9, 9, 3, 9, 3, 5, 3, 8,\n",
       "        3, 5, 9, 5, 9, 5, 3, 9, 3, 8, 9, 5, 9, 5, 9, 5, 8, 8, 9, 8])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class = torch.argmax(preds, axis=1)\n",
    "pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f256a0e6",
   "metadata": {},
   "source": [
    "From this the accuracy can be calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L13){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### accuracy\n",
       "\n",
       ">      accuracy (preds, targets)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L13){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### accuracy\n",
       "\n",
       ">      accuracy (preds, targets)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "34855282",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.09)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a962074c",
   "metadata": {},
   "source": [
    "Not suprisingly, at this point the accuracy is what would be expected for a random prediction.\n",
    "\n",
    "To improve this we need to train the model.  To do that it is necessary to say what learning rate we would like and how many epochs to train for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "246c050e",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "lr = 0.5\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5f925",
   "metadata": {},
   "source": [
    "Now create a simple training loop following the above steps. Note that the loss, backward and parameter update is happening for every batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b033e8d",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3036487102508545, accuracy = 0.09375\n",
      "Loss: 2.2052316665649414, accuracy = 0.296875\n",
      "Loss: 2.1905035972595215, accuracy = 0.21875\n",
      "Loss: 2.010064125061035, accuracy = 0.53125\n",
      "Loss: 1.9125093221664429, accuracy = 0.5\n",
      "Loss: 1.7081104516983032, accuracy = 0.703125\n",
      "Loss: 1.6426200866699219, accuracy = 0.5\n",
      "Loss: 1.6326062679290771, accuracy = 0.625\n",
      "Loss: 1.5491729974746704, accuracy = 0.4375\n",
      "Loss: 1.5245637893676758, accuracy = 0.53125\n",
      "Loss: 0.12374816089868546, accuracy = 0.96875\n",
      "Loss: 0.14763614535331726, accuracy = 0.984375\n",
      "Loss: 0.29614493250846863, accuracy = 0.890625\n",
      "Loss: 0.16717804968357086, accuracy = 0.9375\n",
      "Loss: 0.1868995726108551, accuracy = 0.90625\n",
      "Loss: 0.05588085949420929, accuracy = 0.984375\n",
      "Loss: 0.10521863400936127, accuracy = 0.96875\n",
      "Loss: 0.2990257740020752, accuracy = 0.9375\n",
      "Loss: 0.04911200702190399, accuracy = 1.0\n",
      "Loss: 0.25950485467910767, accuracy = 0.9375\n",
      "Loss: 0.09232573211193085, accuracy = 0.96875\n",
      "Loss: 0.10159078985452652, accuracy = 0.984375\n",
      "Loss: 0.28459155559539795, accuracy = 0.9375\n",
      "Loss: 0.05941237136721611, accuracy = 0.984375\n",
      "Loss: 0.11075811833143234, accuracy = 0.9375\n",
      "Loss: 0.031431593000888824, accuracy = 0.984375\n",
      "Loss: 0.07170089334249496, accuracy = 0.984375\n",
      "Loss: 0.24644441902637482, accuracy = 0.953125\n",
      "Loss: 0.05402196943759918, accuracy = 0.984375\n",
      "Loss: 0.18466435372829437, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    # iterate through batches\n",
    "    for batch in range(0, n, bs):\n",
    "        # get data\n",
    "        s = slice(batch, min(n, batch+bs))\n",
    "        xs = x_train[s]\n",
    "        ys = y_train[s]\n",
    "        # Pass data through the model\n",
    "        preds = model(xs)\n",
    "        # Calculate loss\n",
    "        loss = loss_func(preds, ys)\n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        # Print the loss periodically\n",
    "        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n",
    "        with torch.no_grad():\n",
    "            for l in model.layers:\n",
    "                if hasattr(l, \"weight\"):\n",
    "                    l.weight -= l.weight.grad * lr\n",
    "                    l.bias -= l.bias.grad * lr\n",
    "                    l.weight.grad.zero_()\n",
    "                    l.bias.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92f5825",
   "metadata": {},
   "source": [
    "### Adding parameters and optim\n",
    "\n",
    "The above training loop works but accessing the model parameters us cumbersome since it requires advanced knowledge of the layers.  Pytorch has some methods to allow accessing and modifying layer information in a more straightforward manner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba647fb",
   "metadata": {},
   "source": [
    "#### Parameters \n",
    "Recreate the model with individual layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b03967b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        x = self.l2(self.relu(self.l1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22530e11",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model = Model(m, nh, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d267f570",
   "metadata": {},
   "source": [
    "Examine the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f55cec1",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: l1, Parameters: Linear(in_features=784, out_features=50, bias=True)\n",
      "Layer: relu, Parameters: ReLU()\n",
      "Layer: l2, Parameters: Linear(in_features=50, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model.named_children():\n",
    "    print(f\"Layer: {name}, Parameters: {layer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a6b8d",
   "metadata": {},
   "source": [
    "The layers can be accessed using the name alone, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d4486d5",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=784, out_features=50, bias=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.l1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41159d0",
   "metadata": {},
   "source": [
    "The layer parameters can also be accessed by the model.parameters property, however, this is an iterator and hence needs to be listed through a loop or creation of a list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "57f4e5ad",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00fb8a0f",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.04,  0.03, -0.02,  ...,  0.03,  0.03,  0.00],\n",
      "        [ 0.04,  0.01,  0.03,  ..., -0.01,  0.02,  0.03]], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.01, -0.01], grad_fn=<SliceBackward0>)\n",
      "tensor([[-0.05,  0.02,  0.07,  0.09,  0.10,  0.12, -0.13,  0.10, -0.09, -0.08, -0.12, -0.01,  0.07, -0.00,  0.12, -0.03, -0.07, -0.14,\n",
      "          0.10,  0.13, -0.12,  0.14,  0.12,  0.08, -0.11,  0.03, -0.09,  0.12,  0.01, -0.03,  0.06, -0.00,  0.01, -0.05,  0.11,  0.14,\n",
      "          0.07,  0.05, -0.09, -0.03, -0.01, -0.01,  0.08,  0.02, -0.09, -0.05,  0.03,  0.13, -0.08,  0.13],\n",
      "        [ 0.09, -0.04,  0.00,  0.14, -0.13, -0.06,  0.03, -0.09, -0.11,  0.05,  0.04, -0.02, -0.04, -0.03,  0.01, -0.10, -0.03, -0.02,\n",
      "          0.00,  0.07,  0.10, -0.08, -0.14, -0.02, -0.13, -0.08,  0.07,  0.04, -0.13, -0.13, -0.05,  0.12, -0.08,  0.13,  0.13, -0.10,\n",
      "          0.06,  0.08, -0.13, -0.08, -0.06, -0.11,  0.07,  0.06,  0.09,  0.04, -0.13,  0.04, -0.10, -0.01]], grad_fn=<SliceBackward0>)\n",
      "tensor([-0.01, -0.06], grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9dd4d1",
   "metadata": {},
   "source": [
    "Using parameters it is possible to simplify the code to optimise the weights and biases by looping through teh model parameters.  It is also possible to zero all of the model's gradients with a since call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a18acea1",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        # iterate through batches\n",
    "        for batch in range(0, n, bs):\n",
    "            # get data\n",
    "            s = slice(batch, min(n, batch+bs))\n",
    "            xs = x_train[s]\n",
    "            ys = y_train[s]\n",
    "            # Pass data through the model\n",
    "            preds = model(xs)\n",
    "            # Calculate loss\n",
    "            loss = loss_func(preds, ys)\n",
    "            # Calculate the gradients\n",
    "            loss.backward()\n",
    "            # Print the loss periodically\n",
    "            if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n",
    "            with torch.no_grad():\n",
    "                for params in model.parameters():\n",
    "                    params -= params.grad * lr\n",
    "                model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "12435976",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.309434175491333, accuracy = 0.0625\n",
      "Loss: 2.177271842956543, accuracy = 0.234375\n",
      "Loss: 2.259394645690918, accuracy = 0.078125\n",
      "Loss: 2.0509822368621826, accuracy = 0.578125\n",
      "Loss: 1.9572300910949707, accuracy = 0.34375\n",
      "Loss: 1.7929021120071411, accuracy = 0.71875\n",
      "Loss: 1.6334275007247925, accuracy = 0.65625\n",
      "Loss: 1.5452338457107544, accuracy = 0.625\n",
      "Loss: 1.507757306098938, accuracy = 0.4375\n",
      "Loss: 1.6010795831680298, accuracy = 0.484375\n",
      "Loss: 0.2006874829530716, accuracy = 0.953125\n",
      "Loss: 0.11892185360193253, accuracy = 0.96875\n",
      "Loss: 0.2844753861427307, accuracy = 0.90625\n",
      "Loss: 0.17238563299179077, accuracy = 0.96875\n",
      "Loss: 0.15464989840984344, accuracy = 0.90625\n",
      "Loss: 0.04285932332277298, accuracy = 1.0\n",
      "Loss: 0.12839168310165405, accuracy = 0.96875\n",
      "Loss: 0.3218374252319336, accuracy = 0.953125\n",
      "Loss: 0.09430787712335587, accuracy = 0.96875\n",
      "Loss: 0.3029904365539551, accuracy = 0.953125\n",
      "Loss: 0.18196934461593628, accuracy = 0.9375\n",
      "Loss: 0.0731976106762886, accuracy = 0.984375\n",
      "Loss: 0.29567304253578186, accuracy = 0.9375\n",
      "Loss: 0.0792182981967926, accuracy = 0.984375\n",
      "Loss: 0.09979861974716187, accuracy = 0.953125\n",
      "Loss: 0.028896179050207138, accuracy = 1.0\n",
      "Loss: 0.10094335675239563, accuracy = 0.96875\n",
      "Loss: 0.28324049711227417, accuracy = 0.9375\n",
      "Loss: 0.055503442883491516, accuracy = 0.984375\n",
      "Loss: 0.2787169814109802, accuracy = 0.96875\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda78190",
   "metadata": {},
   "source": [
    "The way that the model parameters are setup is by Pytorch overriding the __setattrib__ method in nn.Module.  The way that this works is as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "19ad0950",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DummyModule():\n",
    "    def __init__(self, n_in, nh, n_out):\n",
    "        self._modules = {}\n",
    "        self.l1 = nn.Linear(n_in,nh)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.l2 = nn.Linear(nh,n_out)\n",
    "        \n",
    "    def __setattr__(self, k,v):\n",
    "        # Assign the layers to the modules dict before calling the set attribute class.  The parameters are \n",
    "        # simply name (k), value (v)\n",
    "        if not k.startswith(\"_\"):\n",
    "            self._modules[k] = v\n",
    "        super().__setattr__(k,v)\n",
    "            \n",
    "    def __repr__(self): \n",
    "        # Setup so that the official string representation of the class instance is a listing of the module\n",
    "        return f\"{self._modules}\"\n",
    "    \n",
    "    def parameters(self):\n",
    "        # Create an iterator to yield all of the model parameters\n",
    "        for layer in self._modules.values():\n",
    "            for param in layer.parameters(): yield(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7938a129",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l1': Linear(in_features=784, out_features=50, bias=True), 'relu': ReLU(), 'l2': Linear(in_features=50, out_features=10, bias=True)}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = DummyModule(m, nh, c)\n",
    "dm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5901fd81",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([50, 784])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "for param in dm.parameters():\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0580577e",
   "metadata": {},
   "source": [
    "This approach won't work as it with lists of layers as things were originally setup. To allow this. to work the layers have to be individually registered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54ed7e80",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "layers = [nn.Linear(m,nh), nn.ReLU(), nn.Linear(nh,10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e682399a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = layers\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            self.add_module(f\"layer_{i}\", layer)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6e9391c9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model = Model(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "44b38d4b",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (layer_0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (layer_1): ReLU()\n",
       "  (layer_2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "27ac82a9",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3222596645355225, accuracy = 0.03125\n",
      "Loss: 2.2330336570739746, accuracy = 0.21875\n",
      "Loss: 2.222136974334717, accuracy = 0.140625\n",
      "Loss: 2.079106330871582, accuracy = 0.421875\n",
      "Loss: 1.958361029624939, accuracy = 0.5625\n",
      "Loss: 1.7925820350646973, accuracy = 0.625\n",
      "Loss: 1.675372838973999, accuracy = 0.515625\n",
      "Loss: 1.6172857284545898, accuracy = 0.609375\n",
      "Loss: 1.4986895322799683, accuracy = 0.546875\n",
      "Loss: 1.5764800310134888, accuracy = 0.515625\n",
      "Loss: 0.16430063545703888, accuracy = 0.953125\n",
      "Loss: 0.12203794717788696, accuracy = 0.984375\n",
      "Loss: 0.3307473063468933, accuracy = 0.890625\n",
      "Loss: 0.21634256839752197, accuracy = 0.921875\n",
      "Loss: 0.17808055877685547, accuracy = 0.921875\n",
      "Loss: 0.04548545181751251, accuracy = 0.984375\n",
      "Loss: 0.13622808456420898, accuracy = 0.96875\n",
      "Loss: 0.32709184288978577, accuracy = 0.9375\n",
      "Loss: 0.09109017997980118, accuracy = 0.984375\n",
      "Loss: 0.27433082461357117, accuracy = 0.953125\n",
      "Loss: 0.09979915618896484, accuracy = 0.96875\n",
      "Loss: 0.08315068483352661, accuracy = 0.984375\n",
      "Loss: 0.27988025546073914, accuracy = 0.90625\n",
      "Loss: 0.1201467365026474, accuracy = 0.953125\n",
      "Loss: 0.14109008014202118, accuracy = 0.9375\n",
      "Loss: 0.01877472549676895, accuracy = 1.0\n",
      "Loss: 0.10770482569932938, accuracy = 0.96875\n",
      "Loss: 0.3286954462528229, accuracy = 0.953125\n",
      "Loss: 0.06740975379943848, accuracy = 0.984375\n",
      "Loss: 0.18299852311611176, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2954fa23",
   "metadata": {},
   "source": [
    "Registering the layers can be done using the nn.ModuleList class:\n",
    "[Link to nn.ModuleList docs](https://pytorch.org/docs/stable/generated/torch.nn.ModuleList.html?highlight=nn+modulelist#torch.nn.ModuleList)\n",
    "\n",
    "Thus this allows further simplification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4f144720",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class SequentialModel(nn.Module):\n",
    "    def __init__(self, layers):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        for l in self.layers: x = l(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "410a07f9",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=50, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = SequentialModel(layers)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5f3c4d2",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.06515892595052719, accuracy = 0.96875\n",
      "Loss: 0.06317746639251709, accuracy = 0.984375\n",
      "Loss: 0.21157535910606384, accuracy = 0.921875\n",
      "Loss: 0.10860879719257355, accuracy = 0.96875\n",
      "Loss: 0.08964528143405914, accuracy = 0.9375\n",
      "Loss: 0.011501152999699116, accuracy = 1.0\n",
      "Loss: 0.08443643152713776, accuracy = 0.96875\n",
      "Loss: 0.3277687132358551, accuracy = 0.9375\n",
      "Loss: 0.06517940014600754, accuracy = 0.984375\n",
      "Loss: 0.17099246382713318, accuracy = 0.96875\n",
      "Loss: 0.04173600673675537, accuracy = 0.984375\n",
      "Loss: 0.04945621266961098, accuracy = 0.984375\n",
      "Loss: 0.1965624988079071, accuracy = 0.90625\n",
      "Loss: 0.0814109668135643, accuracy = 0.96875\n",
      "Loss: 0.06253797560930252, accuracy = 0.96875\n",
      "Loss: 0.009009594097733498, accuracy = 1.0\n",
      "Loss: 0.08011716604232788, accuracy = 0.96875\n",
      "Loss: 0.32447099685668945, accuracy = 0.9375\n",
      "Loss: 0.05112294852733612, accuracy = 0.984375\n",
      "Loss: 0.15682904422283173, accuracy = 0.984375\n",
      "Loss: 0.02698095701634884, accuracy = 1.0\n",
      "Loss: 0.035535961389541626, accuracy = 0.984375\n",
      "Loss: 0.15110039710998535, accuracy = 0.921875\n",
      "Loss: 0.0608980655670166, accuracy = 0.96875\n",
      "Loss: 0.03945880010724068, accuracy = 1.0\n",
      "Loss: 0.005177198443561792, accuracy = 1.0\n",
      "Loss: 0.07748386263847351, accuracy = 0.96875\n",
      "Loss: 0.3055875599384308, accuracy = 0.9375\n",
      "Loss: 0.04133985936641693, accuracy = 0.984375\n",
      "Loss: 0.16388559341430664, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "44eb488b",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.02, grad_fn=<NllLossBackward0>), tensor(1.))"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8687db",
   "metadata": {},
   "source": [
    "#### The nn.Sequential Class\n",
    "\n",
    "This class does what is done above, in other words it takes a list of layers and created a model by registering and then saving the layers in sequence.\n",
    "\n",
    "The sequential class will not accept a list as an input and so we have to pass in the individual layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "409dfe03",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Linear(in_features=784, out_features=50, bias=True)\n",
       "  (1): ReLU()\n",
       "  (2): Linear(in_features=50, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "46786d53",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.3087124824523926, accuracy = 0.09375\n",
      "Loss: 2.220724582672119, accuracy = 0.4375\n",
      "Loss: 2.2094054222106934, accuracy = 0.15625\n",
      "Loss: 2.0799782276153564, accuracy = 0.484375\n",
      "Loss: 1.9859000444412231, accuracy = 0.421875\n",
      "Loss: 1.8239319324493408, accuracy = 0.5625\n",
      "Loss: 1.692657232284546, accuracy = 0.625\n",
      "Loss: 1.5962902307510376, accuracy = 0.671875\n",
      "Loss: 1.4929006099700928, accuracy = 0.46875\n",
      "Loss: 1.5572547912597656, accuracy = 0.53125\n",
      "Loss: 0.2031664103269577, accuracy = 0.90625\n",
      "Loss: 0.12023147940635681, accuracy = 0.984375\n",
      "Loss: 0.36551305651664734, accuracy = 0.890625\n",
      "Loss: 0.1421089768409729, accuracy = 0.96875\n",
      "Loss: 0.13040338456630707, accuracy = 0.921875\n",
      "Loss: 0.03445771709084511, accuracy = 0.984375\n",
      "Loss: 0.14158782362937927, accuracy = 0.96875\n",
      "Loss: 0.29142311215400696, accuracy = 0.953125\n",
      "Loss: 0.1052810475230217, accuracy = 0.953125\n",
      "Loss: 0.22577130794525146, accuracy = 0.984375\n",
      "Loss: 0.20330604910850525, accuracy = 0.921875\n",
      "Loss: 0.1524616777896881, accuracy = 0.96875\n",
      "Loss: 0.3139619827270508, accuracy = 0.921875\n",
      "Loss: 0.10483880341053009, accuracy = 0.953125\n",
      "Loss: 0.09913913160562515, accuracy = 0.9375\n",
      "Loss: 0.02614644728600979, accuracy = 1.0\n",
      "Loss: 0.1354278326034546, accuracy = 0.96875\n",
      "Loss: 0.2590445876121521, accuracy = 0.953125\n",
      "Loss: 0.06993013620376587, accuracy = 0.96875\n",
      "Loss: 0.1877637505531311, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb79f83",
   "metadata": {},
   "source": [
    "#### Introducing the optimiser class Optim\n",
    "\n",
    "So far we have developed the loss function and the model but the appliction of the weights has been done using a relatively simple implmentation of SGD.  The process of optimising the model parameters can be built into a class.  In Pytorch this is the Optim class.  This will now be developed.\n",
    "\n",
    "Note that in the class below there are two things to ensure:\n",
    "1. The params are converted into a list in the init function (to facilitate iteration)\n",
    "2. when zeroing the gradients it is important to use the in place version of the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8a187041",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Optimizer():\n",
    "    def __init__(self, params, lr=0.5):\n",
    "        self.params = list(params); self.lr = lr\n",
    "        \n",
    "    def step(self):\n",
    "        with torch.no_grad():\n",
    "            for param in self.params: param -= param.grad * self.lr\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.params: param.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "31ebac49",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "214467aa",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "opt = Optimizer(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8febbdcb",
   "metadata": {},
   "source": [
    "Update the training loop to work with the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c9f6f48",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.30017352104187, accuracy = 0.0625\n",
      "Loss: 2.1758975982666016, accuracy = 0.234375\n",
      "Loss: 2.199198007583618, accuracy = 0.1875\n",
      "Loss: 2.005563497543335, accuracy = 0.453125\n",
      "Loss: 1.9248197078704834, accuracy = 0.359375\n",
      "Loss: 1.772184133529663, accuracy = 0.578125\n",
      "Loss: 1.62507963180542, accuracy = 0.546875\n",
      "Loss: 1.5354200601577759, accuracy = 0.640625\n",
      "Loss: 1.5023629665374756, accuracy = 0.46875\n",
      "Loss: 1.6080732345581055, accuracy = 0.421875\n",
      "Loss: 0.13067957758903503, accuracy = 0.96875\n",
      "Loss: 0.1198587417602539, accuracy = 0.984375\n",
      "Loss: 0.2864210307598114, accuracy = 0.921875\n",
      "Loss: 0.21201932430267334, accuracy = 0.921875\n",
      "Loss: 0.1917288601398468, accuracy = 0.921875\n",
      "Loss: 0.04466291889548302, accuracy = 0.96875\n",
      "Loss: 0.132561594247818, accuracy = 0.96875\n",
      "Loss: 0.3979755938053131, accuracy = 0.9375\n",
      "Loss: 0.10060809552669525, accuracy = 0.953125\n",
      "Loss: 0.3057703375816345, accuracy = 0.9375\n",
      "Loss: 0.12934750318527222, accuracy = 0.96875\n",
      "Loss: 0.08657151460647583, accuracy = 0.96875\n",
      "Loss: 0.21188294887542725, accuracy = 0.90625\n",
      "Loss: 0.14039435982704163, accuracy = 0.96875\n",
      "Loss: 0.09377827495336533, accuracy = 0.984375\n",
      "Loss: 0.0185707900673151, accuracy = 1.0\n",
      "Loss: 0.081519216299057, accuracy = 0.96875\n",
      "Loss: 0.32961714267730713, accuracy = 0.953125\n",
      "Loss: 0.051018428057432175, accuracy = 1.0\n",
      "Loss: 0.21368607878684998, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in range(0, n, bs):\n",
    "        # get data\n",
    "        s = slice(batch, min(n, batch+bs))\n",
    "        xs = x_train[s]\n",
    "        ys = y_train[s]\n",
    "        # Pass data through the model\n",
    "        preds = model(xs)\n",
    "        loss = loss_func(preds, ys)\n",
    "        # Print results\n",
    "        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Apply the optimiser\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ba2ce5",
   "metadata": {},
   "source": [
    "The same can be achieved by the use of the pytorch optim library, specifically in this case optim.SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ebc6c4f2",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a7602fa9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_model_and_optimizer():\n",
    "    model = nn.Sequential(nn.Linear(m, nh), nn.ReLU(), nn.Linear(nh, c))\n",
    "    opt = optim.SGD(model.parameters(), lr=0.5)\n",
    "    return model, opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f8ffc5be",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.31, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, opt = get_model_and_optimizer()\n",
    "loss_func(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4069df80",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.312685012817383, accuracy = 0.078125\n",
      "Loss: 2.1906702518463135, accuracy = 0.265625\n",
      "Loss: 2.1999802589416504, accuracy = 0.1875\n",
      "Loss: 2.0161678791046143, accuracy = 0.546875\n",
      "Loss: 1.8945502042770386, accuracy = 0.421875\n",
      "Loss: 1.7321070432662964, accuracy = 0.640625\n",
      "Loss: 1.5854045152664185, accuracy = 0.640625\n",
      "Loss: 1.4941891431808472, accuracy = 0.671875\n",
      "Loss: 1.4101829528808594, accuracy = 0.546875\n",
      "Loss: 1.4672781229019165, accuracy = 0.5\n",
      "Loss: 0.2142210453748703, accuracy = 0.90625\n",
      "Loss: 0.14814303815364838, accuracy = 0.96875\n",
      "Loss: 0.32211652398109436, accuracy = 0.90625\n",
      "Loss: 0.1859450340270996, accuracy = 0.953125\n",
      "Loss: 0.12843002378940582, accuracy = 0.9375\n",
      "Loss: 0.05411830171942711, accuracy = 0.984375\n",
      "Loss: 0.13670694828033447, accuracy = 0.96875\n",
      "Loss: 0.3170986473560333, accuracy = 0.953125\n",
      "Loss: 0.10294653475284576, accuracy = 0.953125\n",
      "Loss: 0.1652507185935974, accuracy = 0.953125\n",
      "Loss: 0.178289994597435, accuracy = 0.921875\n",
      "Loss: 0.10243190824985504, accuracy = 0.96875\n",
      "Loss: 0.2527705132961273, accuracy = 0.921875\n",
      "Loss: 0.13069084286689758, accuracy = 0.953125\n",
      "Loss: 0.09603864699602127, accuracy = 0.953125\n",
      "Loss: 0.02239409275352955, accuracy = 1.0\n",
      "Loss: 0.1111244335770607, accuracy = 0.96875\n",
      "Loss: 0.2913336157798767, accuracy = 0.953125\n",
      "Loss: 0.04398681968450546, accuracy = 1.0\n",
      "Loss: 0.1203981563448906, accuracy = 0.96875\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in range(0, n, bs):\n",
    "        # get data\n",
    "        s = slice(batch, min(n, batch+bs))\n",
    "        xs = x_train[s]\n",
    "        ys = y_train[s]\n",
    "        # Pass data through the model\n",
    "        preds = model(xs)\n",
    "        loss = loss_func(preds, ys)\n",
    "        # Print results\n",
    "        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Apply the optimiser\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10db2cad",
   "metadata": {},
   "source": [
    "#### Dataset and DataLoader\n",
    "\n",
    "The next part of the development is to make the dataloading more generic, faster and more robust.  This is done through the creation of datasets and dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfb3a02",
   "metadata": {},
   "source": [
    "##### Dataset\n",
    "\n",
    "It's clunky to iterate through minibatches of x and y values separately:\n",
    "\n",
    "```python\n",
    "    xb = x_train[s]\n",
    "    yb = y_train[s]\n",
    "```\n",
    "\n",
    "Instead, let's do these two steps together, by introducing a [`Dataset`](https://fastai.github.io/fastai-course-22p/dataloaders_optimisers_training.html#dataset) class:\n",
    "\n",
    "```python\n",
    "    xb,yb = train_ds[s]\n",
    "```\n",
    "\n",
    "In essence the dataset class is a way to robustly link inputs and targets through index values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Dataset\n",
       "\n",
       ">      Dataset (x, y)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L17){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### Dataset\n",
       "\n",
       ">      Dataset (x, y)\n",
       "\n",
       "Initialize self.  See help(type(self)) for accurate signature."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "881d52a2",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_ds, valid_ds = Dataset(x_train, y_train), Dataset(x_valid, y_valid)\n",
    "assert len(x_train) == len(train_ds)\n",
    "assert len(x_valid) == len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f5d15262",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "datum_xb, datum_yb = x_train[0:bs], y_train[0:bs]\n",
    "xb, yb = train_ds[0:bs]\n",
    "assert datum_xb.shape == xb.shape\n",
    "assert datum_yb.shape == yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8e720e4a",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb[0:3], yb[0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a544c3b",
   "metadata": {},
   "source": [
    "Now adapt the training loop to use the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0ed2708d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model, opt = get_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ea382c2",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 2.2979648113250732, accuracy = 0.09375\n",
      "Loss: 2.1677284240722656, accuracy = 0.40625\n",
      "Loss: 2.192817211151123, accuracy = 0.171875\n",
      "Loss: 1.992324709892273, accuracy = 0.5\n",
      "Loss: 1.8881453275680542, accuracy = 0.5\n",
      "Loss: 1.6967313289642334, accuracy = 0.703125\n",
      "Loss: 1.6032072305679321, accuracy = 0.578125\n",
      "Loss: 1.5213433504104614, accuracy = 0.65625\n",
      "Loss: 1.4437696933746338, accuracy = 0.46875\n",
      "Loss: 1.5202784538269043, accuracy = 0.53125\n",
      "Loss: 0.2117222249507904, accuracy = 0.953125\n",
      "Loss: 0.15176746249198914, accuracy = 0.96875\n",
      "Loss: 0.2823413908481598, accuracy = 0.921875\n",
      "Loss: 0.1383940577507019, accuracy = 0.953125\n",
      "Loss: 0.13499413430690765, accuracy = 0.921875\n",
      "Loss: 0.04650742560625076, accuracy = 0.984375\n",
      "Loss: 0.12225606292486191, accuracy = 0.953125\n",
      "Loss: 0.31971240043640137, accuracy = 0.953125\n",
      "Loss: 0.07508628815412521, accuracy = 0.96875\n",
      "Loss: 0.22737926244735718, accuracy = 0.96875\n",
      "Loss: 0.1587153971195221, accuracy = 0.921875\n",
      "Loss: 0.12036815285682678, accuracy = 0.984375\n",
      "Loss: 0.27983012795448303, accuracy = 0.90625\n",
      "Loss: 0.07115809619426727, accuracy = 0.984375\n",
      "Loss: 0.10483404994010925, accuracy = 0.9375\n",
      "Loss: 0.025242304429411888, accuracy = 1.0\n",
      "Loss: 0.09612352401018143, accuracy = 0.953125\n",
      "Loss: 0.2911374866962433, accuracy = 0.953125\n",
      "Loss: 0.05527857318520546, accuracy = 0.984375\n",
      "Loss: 0.16760416328907013, accuracy = 0.984375\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    for batch in range(0, n, bs):\n",
    "        # get data\n",
    "        xs, ys = train_ds[batch: min(n, batch+bs)]\n",
    "        # Pass data through the model\n",
    "        preds = model(xs)\n",
    "        loss = loss_func(preds, ys)\n",
    "        # Print results\n",
    "        if batch // bs // 10 == 0: print(f\"Loss: {loss.item()}, accuracy = {accuracy(preds, ys).item()}\")\n",
    "        # Calculate gradients\n",
    "        loss.backward()\n",
    "        # Apply the optimiser\n",
    "        opt.step()\n",
    "        opt.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143df375",
   "metadata": {},
   "source": [
    "##### DataLoader\n",
    "\n",
    "The dataloader takes responsibility for which data to load, which allows choices to me made such as whether to sample randomly or sequentially etc.  Effectively the dataloader is an iterator that will feed data one batch at a time until the dataset is exhausted.\n",
    "\n",
    "In the example below a simple sequential sampler it implemented, others will be added later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4659dc28",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, bs):\n",
    "        self.dataset, self.bs = dataset, bs\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for i in range(0, len(self.dataset), self.bs):\n",
    "            yield self.dataset[i:min(i+self.bs, len(self.dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "9a181df4",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6583ca75",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs)\n",
    "valid_dl = DataLoader(valid_ds, bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c7009c6f",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "xb, yb = next(iter(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2a9a2cc2",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2a37b9de",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(5)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/av/WaAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbe0lEQVR4nO3df2xV9f3H8dflR6+I7e1KbW8rPyygsIlgxqDrVMRRKd1G5McWdS7BzWhwrRGYuNRM0W2uDqczbEz5Y4GxCSjJgEEWNi22ZLNgQBgxbg0l3VpGWyZb7y2FFmw/3z+I98uVFjyXe/u+vTwfySeh955378fjtU9vezn1OeecAADoZ4OsNwAAuDIRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYGKI9QY+qaenR8eOHVN6erp8Pp/1dgAAHjnn1N7ervz8fA0a1PfrnKQL0LFjxzRq1CjrbQAALlNTU5NGjhzZ5/1J9y249PR06y0AAOLgUl/PExag1atX6/rrr9dVV12lwsJCvfvuu59qjm+7AUBquNTX84QE6PXXX9eyZcu0YsUKvffee5oyZYpKSkp0/PjxRDwcAGAgcgkwffp0V1ZWFvm4u7vb5efnu8rKykvOhkIhJ4nFYrFYA3yFQqGLfr2P+yugM2fOaP/+/SouLo7cNmjQIBUXF6u2tvaC47u6uhQOh6MWACD1xT1AH374obq7u5Wbmxt1e25urlpaWi44vrKyUoFAILJ4BxwAXBnM3wVXUVGhUCgUWU1NTdZbAgD0g7j/PaDs7GwNHjxYra2tUbe3trYqGAxecLzf75ff74/3NgAASS7ur4DS0tI0depUVVVVRW7r6elRVVWVioqK4v1wAIABKiFXQli2bJkWLVqkL3zhC5o+fbpefvlldXR06Nvf/nYiHg4AMAAlJED33HOP/vOf/+jpp59WS0uLbrnlFu3cufOCNyYAAK5cPuecs97E+cLhsAKBgPU2AACXKRQKKSMjo8/7zd8FBwC4MhEgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmhlhvAEgmgwcP9jwTCAQSsJP4KC8vj2nu6quv9jwzYcIEzzNlZWWeZ372s595nrnvvvs8z0hSZ2en55nnn3/e88yzzz7reSYV8AoIAGCCAAEATMQ9QM8884x8Pl/UmjhxYrwfBgAwwCXkZ0A33XST3nrrrf9/kCH8qAkAEC0hZRgyZIiCwWAiPjUAIEUk5GdAhw8fVn5+vsaOHav7779fjY2NfR7b1dWlcDgctQAAqS/uASosLNS6deu0c+dOvfLKK2poaNDtt9+u9vb2Xo+vrKxUIBCIrFGjRsV7SwCAJBT3AJWWluob3/iGJk+erJKSEv3xj39UW1ub3njjjV6Pr6ioUCgUiqympqZ4bwkAkIQS/u6AzMxM3Xjjjaqvr+/1fr/fL7/fn+htAACSTML/HtDJkyd15MgR5eXlJfqhAAADSNwD9Pjjj6umpkb//Oc/9c4772j+/PkaPHhwzJfCAACkprh/C+7o0aO67777dOLECV177bW67bbbtGfPHl177bXxfigAwAAW9wBt2rQp3p8SSWr06NGeZ9LS0jzPfOlLX/I8c9ttt3mekc79zNKrhQsXxvRYqebo0aOeZ1atWuV5Zv78+Z5n+noX7qX87W9/8zxTU1MT02NdibgWHADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgwuecc9abOF84HFYgELDexhXllltuiWlu165dnmf4dzsw9PT0eJ75zne+43nm5MmTnmdi0dzcHNPc//73P88zdXV1MT1WKgqFQsrIyOjzfl4BAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwMQQ6w3AXmNjY0xzJ06c8DzD1bDP2bt3r+eZtrY2zzN33nmn5xlJOnPmjOeZ3/72tzE9Fq5cvAICAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAExwMVLov//9b0xzy5cv9zzzta99zfPMgQMHPM+sWrXK80ysDh486Hnmrrvu8jzT0dHheeamm27yPCNJjz32WExzgBe8AgIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATPicc856E+cLh8MKBALW20CCZGRkeJ5pb2/3PLNmzRrPM5L04IMPep751re+5Xlm48aNnmeAgSYUCl30v3leAQEATBAgAIAJzwHavXu35s6dq/z8fPl8Pm3dujXqfuecnn76aeXl5WnYsGEqLi7W4cOH47VfAECK8Bygjo4OTZkyRatXr+71/pUrV2rVqlV69dVXtXfvXg0fPlwlJSXq7Oy87M0CAFKH59+IWlpaqtLS0l7vc87p5Zdf1g9+8APdfffdkqT169crNzdXW7du1b333nt5uwUApIy4/gyooaFBLS0tKi4ujtwWCARUWFio2traXme6uroUDoejFgAg9cU1QC0tLZKk3NzcqNtzc3Mj931SZWWlAoFAZI0aNSqeWwIAJCnzd8FVVFQoFApFVlNTk/WWAAD9IK4BCgaDkqTW1tao21tbWyP3fZLf71dGRkbUAgCkvrgGqKCgQMFgUFVVVZHbwuGw9u7dq6Kiong+FABggPP8LriTJ0+qvr4+8nFDQ4MOHjyorKwsjR49WkuWLNGPf/xj3XDDDSooKNBTTz2l/Px8zZs3L577BgAMcJ4DtG/fPt15552Rj5ctWyZJWrRokdatW6cnnnhCHR0devjhh9XW1qbbbrtNO3fu1FVXXRW/XQMABjwuRoqU9MILL8Q09/H/UHlRU1Pjeeb8v6rwafX09HieASxxMVIAQFIiQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACa6GjZQ0fPjwmOa2b9/ueeaOO+7wPFNaWup55s9//rPnGcASV8MGACQlAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEFyMFzjNu3DjPM++9957nmba2Ns8zb7/9tueZffv2eZ6RpNWrV3ueSbIvJUgCXIwUAJCUCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATXIwUuEzz58/3PLN27VrPM+np6Z5nYvXkk096nlm/fr3nmebmZs8zGDi4GCkAICkRIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACa4GClgYNKkSZ5nXnrpJc8zs2bN8jwTqzVr1nieee655zzP/Pvf//Y8AxtcjBQAkJQIEADAhOcA7d69W3PnzlV+fr58Pp+2bt0adf8DDzwgn88XtebMmROv/QIAUoTnAHV0dGjKlClavXp1n8fMmTNHzc3NkbVx48bL2iQAIPUM8TpQWlqq0tLSix7j9/sVDAZj3hQAIPUl5GdA1dXVysnJ0YQJE/TII4/oxIkTfR7b1dWlcDgctQAAqS/uAZozZ47Wr1+vqqoq/fSnP1VNTY1KS0vV3d3d6/GVlZUKBAKRNWrUqHhvCQCQhDx/C+5S7r333sifb775Zk2ePFnjxo1TdXV1r38noaKiQsuWLYt8HA6HiRAAXAES/jbssWPHKjs7W/X19b3e7/f7lZGREbUAAKkv4QE6evSoTpw4oby8vEQ/FABgAPH8LbiTJ09GvZppaGjQwYMHlZWVpaysLD377LNauHChgsGgjhw5oieeeELjx49XSUlJXDcOABjYPAdo3759uvPOOyMff/zzm0WLFumVV17RoUOH9Jvf/EZtbW3Kz8/X7Nmz9aMf/Uh+vz9+uwYADHhcjBQYIDIzMz3PzJ07N6bHWrt2recZn8/neWbXrl2eZ+666y7PM7DBxUgBAEmJAAEATBAgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJrgaNoALdHV1eZ4ZMsTzb3fRRx995Hkmlt8tVl1d7XkGl4+rYQMAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYML71QMBXLbJkyd7nvn617/ueWbatGmeZ6TYLiwaiw8++MDzzO7duxOwE1jgFRAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIKLkQLnmTBhgueZ8vJyzzMLFizwPBMMBj3P9Kfu7m7PM83NzZ5nenp6PM8gOfEKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVIkfRiuQjnfffdF9NjxXJh0euvvz6mx0pm+/bt8zzz3HPPeZ75wx/+4HkGqYNXQAAAEwQIAGDCU4AqKys1bdo0paenKycnR/PmzVNdXV3UMZ2dnSorK9OIESN0zTXXaOHChWptbY3rpgEAA5+nANXU1KisrEx79uzRm2++qbNnz2r27Nnq6OiIHLN06VJt375dmzdvVk1NjY4dOxbTL98CAKQ2T29C2LlzZ9TH69atU05Ojvbv368ZM2YoFArp17/+tTZs2KAvf/nLkqS1a9fqs5/9rPbs2aMvfvGL8ds5AGBAu6yfAYVCIUlSVlaWJGn//v06e/asiouLI8dMnDhRo0ePVm1tba+fo6urS+FwOGoBAFJfzAHq6enRkiVLdOutt2rSpEmSpJaWFqWlpSkzMzPq2NzcXLW0tPT6eSorKxUIBCJr1KhRsW4JADCAxBygsrIyvf/++9q0adNlbaCiokKhUCiympqaLuvzAQAGhpj+Imp5ebl27Nih3bt3a+TIkZHbg8Ggzpw5o7a2tqhXQa2trX3+ZUK/3y+/3x/LNgAAA5inV0DOOZWXl2vLli3atWuXCgoKou6fOnWqhg4dqqqqqshtdXV1amxsVFFRUXx2DABICZ5eAZWVlWnDhg3atm2b0tPTIz/XCQQCGjZsmAKBgB588EEtW7ZMWVlZysjI0KOPPqqioiLeAQcAiOIpQK+88ookaebMmVG3r127Vg888IAk6ec//7kGDRqkhQsXqqurSyUlJfrVr34Vl80CAFKHzznnrDdxvnA4rEAgYL0NfAq5ubmeZz73uc95nvnlL3/peWbixImeZ5Ld3r17Pc+88MILMT3Wtm3bPM/09PTE9FhIXaFQSBkZGX3ez7XgAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYCKm34iK5JWVleV5Zs2aNTE91i233OJ5ZuzYsTE9VjJ75513PM+8+OKLnmf+9Kc/eZ45ffq05xmgv/AKCABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwwcVI+0lhYaHnmeXLl3uemT59uueZ6667zvNMsjt16lRMc6tWrfI885Of/MTzTEdHh+cZINXwCggAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMMHFSPvJ/Pnz+2WmP33wwQeeZ3bs2OF55qOPPvI88+KLL3qekaS2traY5gB4xysgAIAJAgQAMEGAAAAmCBAAwAQBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMCEzznnrDdxvnA4rEAgYL0NAMBlCoVCysjI6PN+XgEBAEwQIACACU8Bqqys1LRp05Senq6cnBzNmzdPdXV1UcfMnDlTPp8vai1evDiumwYADHyeAlRTU6OysjLt2bNHb775ps6ePavZs2ero6Mj6riHHnpIzc3NkbVy5cq4bhoAMPB5+o2oO3fujPp43bp1ysnJ0f79+zVjxozI7VdffbWCwWB8dggASEmX9TOgUCgkScrKyoq6/bXXXlN2drYmTZqkiooKnTp1qs/P0dXVpXA4HLUAAFcAF6Pu7m731a9+1d16661Rt69Zs8bt3LnTHTp0yP3ud79z1113nZs/f36fn2fFihVOEovFYrFSbIVCoYt2JOYALV682I0ZM8Y1NTVd9LiqqionydXX1/d6f2dnpwuFQpHV1NRkftJYLBaLdfnrUgHy9DOgj5WXl2vHjh3avXu3Ro4cedFjCwsLJUn19fUaN27cBff7/X75/f5YtgEAGMA8Bcg5p0cffVRbtmxRdXW1CgoKLjlz8OBBSVJeXl5MGwQApCZPASorK9OGDRu0bds2paenq6WlRZIUCAQ0bNgwHTlyRBs2bNBXvvIVjRgxQocOHdLSpUs1Y8YMTZ48OSH/AACAAcrLz33Ux/f51q5d65xzrrGx0c2YMcNlZWU5v9/vxo8f75YvX37J7wOeLxQKmX/fksVisViXvy71tZ+LkQIAEoKLkQIAkhIBAgCYIEAAABMECABgggABAEwQIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACYIEADABAECAJggQAAAEwQIAGCCAAEATBAgAIAJAgQAMEGAAAAmCBAAwETSBcg5Z70FAEAcXOrredIFqL293XoLAIA4uNTXc59LspccPT09OnbsmNLT0+Xz+aLuC4fDGjVqlJqampSRkWG0Q3uch3M4D+dwHs7hPJyTDOfBOaf29nbl5+dr0KC+X+cM6cc9fSqDBg3SyJEjL3pMRkbGFf0E+xjn4RzOwzmch3M4D+dYn4dAIHDJY5LuW3AAgCsDAQIAmBhQAfL7/VqxYoX8fr/1VkxxHs7hPJzDeTiH83DOQDoPSfcmBADAlWFAvQICAKQOAgQAMEGAAAAmCBAAwMSACdDq1at1/fXX66qrrlJhYaHeffdd6y31u2eeeUY+ny9qTZw40XpbCbd7927NnTtX+fn58vl82rp1a9T9zjk9/fTTysvL07Bhw1RcXKzDhw/bbDaBLnUeHnjggQueH3PmzLHZbIJUVlZq2rRpSk9PV05OjubNm6e6urqoYzo7O1VWVqYRI0bommuu0cKFC9Xa2mq048T4NOdh5syZFzwfFi9ebLTj3g2IAL3++utatmyZVqxYoffee09TpkxRSUmJjh8/br21fnfTTTepubk5sv7yl79YbynhOjo6NGXKFK1evbrX+1euXKlVq1bp1Vdf1d69ezV8+HCVlJSos7Ozn3eaWJc6D5I0Z86cqOfHxo0b+3GHiVdTU6OysjLt2bNHb775ps6ePavZs2ero6MjcszSpUu1fft2bd68WTU1NTp27JgWLFhguOv4+zTnQZIeeuihqOfDypUrjXbcBzcATJ8+3ZWVlUU+7u7udvn5+a6ystJwV/1vxYoVbsqUKdbbMCXJbdmyJfJxT0+PCwaD7oUXXojc1tbW5vx+v9u4caPBDvvHJ8+Dc84tWrTI3X333Sb7sXL8+HEnydXU1Djnzv27Hzp0qNu8eXPkmL///e9OkqutrbXaZsJ98jw459wdd9zhHnvsMbtNfQpJ/wrozJkz2r9/v4qLiyO3DRo0SMXFxaqtrTXcmY3Dhw8rPz9fY8eO1f3336/GxkbrLZlqaGhQS0tL1PMjEAiosLDwinx+VFdXKycnRxMmTNAjjzyiEydOWG8poUKhkCQpKytLkrR//36dPXs26vkwceJEjR49OqWfD588Dx977bXXlJ2drUmTJqmiokKnTp2y2F6fku5ipJ/04Ycfqru7W7m5uVG35+bm6h//+IfRrmwUFhZq3bp1mjBhgpqbm/Xss8/q9ttv1/vvv6/09HTr7ZloaWmRpF6fHx/fd6WYM2eOFixYoIKCAh05ckRPPvmkSktLVVtbq8GDB1tvL+56enq0ZMkS3XrrrZo0aZKkc8+HtLQ0ZWZmRh2bys+H3s6DJH3zm9/UmDFjlJ+fr0OHDun73/++6urq9Pvf/95wt9GSPkD4f6WlpZE/T548WYWFhRozZozeeOMNPfjgg4Y7QzK49957I3+++eabNXnyZI0bN07V1dWaNWuW4c4So6ysTO+///4V8XPQi+nrPDz88MORP998883Ky8vTrFmzdOTIEY0bN66/t9mrpP8WXHZ2tgYPHnzBu1haW1sVDAaNdpUcMjMzdeONN6q+vt56K2Y+fg7w/LjQ2LFjlZ2dnZLPj/Lycu3YsUNvv/121K9vCQaDOnPmjNra2qKOT9XnQ1/noTeFhYWSlFTPh6QPUFpamqZOnaqqqqrIbT09PaqqqlJRUZHhzuydPHlSR44cUV5envVWzBQUFCgYDEY9P8LhsPbu3XvFPz+OHj2qEydOpNTzwzmn8vJybdmyRbt27VJBQUHU/VOnTtXQoUOjng91dXVqbGxMqefDpc5Dbw4ePChJyfV8sH4XxKexadMm5/f73bp169wHH3zgHn74YZeZmelaWlqst9avvve977nq6mrX0NDg/vrXv7ri4mKXnZ3tjh8/br21hGpvb3cHDhxwBw4ccJLcSy+95A4cOOD+9a9/Oeece/75511mZqbbtm2bO3TokLv77rtdQUGBO336tPHO4+ti56G9vd09/vjjrra21jU0NLi33nrLff7zn3c33HCD6+zstN563DzyyCMuEAi46upq19zcHFmnTp2KHLN48WI3evRot2vXLrdv3z5XVFTkioqKDHcdf5c6D/X19e6HP/yh27dvn2toaHDbtm1zY8eOdTNmzDDeebQBESDnnPvFL37hRo8e7dLS0tz06dPdnj17rLfU7+655x6Xl5fn0tLS3HXXXefuueceV19fb72thHv77bedpAvWokWLnHPn3or91FNPudzcXOf3+92sWbNcXV2d7aYT4GLn4dSpU2727Nnu2muvdUOHDnVjxoxxDz30UMr9T1pv//yS3Nq1ayPHnD592n33u991n/nMZ9zVV1/t5s+f75qbm+02nQCXOg+NjY1uxowZLisry/n9fjd+/Hi3fPlyFwqFbDf+Cfw6BgCAiaT/GRAAIDURIACACQIEADBBgAAAJggQAMAEAQIAmCBAAAATBAgAYIIAAQBMECAAgAkCBAAwQYAAACb+Dwuo74MxItlsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f9e25a",
   "metadata": {},
   "source": [
    "Now implement a training loop with the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "713b7825",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model, opt = get_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a80898",
   "metadata": {},
   "source": [
    "Now create a function for the fit process since this will be used several times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d624246b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def fit():\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_dl:\n",
    "            # Pass data through the model\n",
    "            preds = model(xb)\n",
    "            loss = loss_func(preds, yb)\n",
    "            # Calculate gradients\n",
    "            loss.backward()\n",
    "            # Apply the optimiser\n",
    "            opt.step()\n",
    "            opt.zero_grad()\n",
    "        print(f'After epoch {epoch}, batch loss is: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4431a8dd",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0, batch loss is: 0.3089807331562042\n",
      "After epoch 1, batch loss is: 0.19600419700145721\n",
      "After epoch 2, batch loss is: 0.10103154182434082\n"
     ]
    }
   ],
   "source": [
    "fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "e5d13e0e",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.17, grad_fn=<NllLossBackward0>), tensor(0.94))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f0bd28e0",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09647142",
   "metadata": {},
   "source": [
    "### Adding in different sampling methods\n",
    "\n",
    "To enable random or linear sampling a class is added to manage the sampling.  This will now be developed.  The inputs to the sampler need to be the dataset and a flag to indicate the type of sampling.  The return should be the same dataset either shuffled or processed as necessary.  This could be done upon just the item index values or the whole dataset could be processed and returned.  In this case the item indexies are returned as a list as requested by the iter call\n",
    "\n",
    "I'm not sure why yield is not used here - something to check up on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8fde1e5a",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "82d40b32",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class Sampler():\n",
    "    def __init__(self, ds, shuffle=False):\n",
    "        self.n = len(ds)\n",
    "        self.ds = ds\n",
    "        self.shuffle = shuffle\n",
    "    \n",
    "    def __iter__(self):\n",
    "        indecies = list(range(self.n))\n",
    "        if self.shuffle:\n",
    "            random.shuffle(indecies)\n",
    "        return iter(indecies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6828e235",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6408c7d5",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = Sampler(x_train, shuffle=False)\n",
    "iterator = iter(sampler)\n",
    "ids = []\n",
    "for i in range(5): ids.append(next(iterator))\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13e0735",
   "metadata": {},
   "source": [
    "This is returning things one item at a time from the iterator.  It is possible to use islice to generate a range of items.  Note that since this iterator has already returned five entries it will supply the next 5, it doesn't start from to front of the list unless a new iterator is created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "14baad51",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = list(islice(iterator, 0, 5))\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9d06eb",
   "metadata": {},
   "source": [
    "When the random flag is set to true then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "dd8c7354",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[19149, 3623, 33271, 45722, 34626, 44572, 33273, 31591, 1328, 44705]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler = Sampler(x_train, shuffle=True)\n",
    "list(islice(sampler, 0, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8b2c2aa6",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampler_v = Sampler(x_valid, shuffle=False)\n",
    "list(islice(sampler_v, 0, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e9f0b7",
   "metadata": {},
   "source": [
    "Having proven the basics of the sampler the fastcore library will be used to create a batch sampler. Two fastcore methods are used:\n",
    "1. store_attr.  This simply saves any parameters supplied as class properties with the same name as the supplied parameter\n",
    "2. chunked. This returns batches of indecies from an iterator of user defined size with the option to specity whether to drop the last chunk if not the batch size, and also to provide only a defined number of chunks (useful to use a part of a dataset when get things working)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d3149377",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import fastcore.all as fc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7628378",
   "metadata": {},
   "source": [
    "The class BatchSampler is require to take as input the dataset, batch size, the sampler, and whether to drop the last batch if not the correct size.  In this case the dataset is already embedded into the sampler and hence does not need to be added separately. \n",
    "The output should be an iterator returning batches of indecies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7dc23bbb",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class BatchSampler:\n",
    "    def __init__(self, sampler, bs=16, drop_last=False):\n",
    "        fc.store_attr()\n",
    "    \n",
    "    def __iter__(self, ):\n",
    "        yield from fc.chunked(iter(self.sampler), chunk_sz=self.bs, drop_last=self.drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "9e7e88dd",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "batches = BatchSampler(sampler, bs=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4906ebde",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[25976, 3805, 2452, 7221]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "77d81002",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[43889, 38760, 1660, 43812],\n",
       " [19474, 27881, 1648, 5994],\n",
       " [43292, 20760, 47927, 46678],\n",
       " [16207, 19001, 31035, 2216],\n",
       " [21505, 234, 47366, 36409]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(islice(batches, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47403ce",
   "metadata": {},
   "source": [
    "BatchSampler will provide a list of ids with which to make up the batch.  The Sampler will then return the individual input data and targets for each id.  The then have to collate the inputs and targets into torch arrays for processing.\n",
    "\n",
    "This collation and stacking of data is done by a collate function\n",
    "\n",
    "We now need a collate method to take the samples and convert them into a stacked tensor or inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d75fc36",
   "metadata": {},
   "source": [
    "Collator inputs: list of tuples of input value pairs\n",
    "outputs: tuple of input and target values as torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df6f34f8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def collate(b):\n",
    "    # the * means that multiple items will be received, which effectively means the list is taken an item\n",
    "    # at a time\n",
    "    xb, yb = zip(*b)\n",
    "    return (torch.stack(xb), torch.stack(yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ac67bbbc",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([5, 0, 4]))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi = [train_ds[0], train_ds[1], train_ds[2]]\n",
    "collate(bi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2119f5a",
   "metadata": {},
   "source": [
    "#### Finally create a full dataloader using the above components\n",
    "\n",
    "inputs:dataset, batch size, sampling method to use, whether to use last batch\n",
    "\n",
    "outputs: tuple of stacked array of inputs and targets\n",
    "\n",
    "methods: initiation - setup the dataset, sampling method, batch size, options\n",
    "         iter - return a batch of data as a tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "de7a18f9",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, collate_fn=collate, bs=64, shuffle=False, drop_last=False):\n",
    "        fc.store_attr()\n",
    "        sampler = Sampler(ds=dataset, shuffle=shuffle)\n",
    "        self.batch_sampler = BatchSampler(sampler, bs, drop_last)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        yield from (self.collate_fn(self.dataset[i] for i in b) for b in self.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7e9535d3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, collate, bs=64, shuffle=True)\n",
    "valid_dl = DataLoader(valid_ds, collate, bs=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "69003b1c",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_batch = next(iter(train_dl))\n",
    "train_batch[0].shape, train_batch[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b41357e5",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 784])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e69376d0",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "xb,yb = next(iter(valid_dl))\n",
    "plt.imshow(xb[0].view(28,28))\n",
    "yb[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "2125ab82",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0, batch loss is: 0.05507710948586464\n",
      "After epoch 1, batch loss is: 0.055910542607307434\n",
      "After epoch 2, batch loss is: 0.07595346868038177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.10, grad_fn=<NllLossBackward0>), tensor(0.97))"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the accuracy of the model when trained with the Dataloader\n",
    "model,opt = get_model_and_optimizer()\n",
    "fit()\n",
    "\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43bc427b",
   "metadata": {},
   "source": [
    "### Multiprocesssing DataLoader\n",
    "Data loading is often a constraint in terms or time to process a job.  Fortunately this is a task that lends itself to multi-processing and hence it makes sense to apply this.\n",
    "\n",
    "Pytorch provide a multi-processing library and that is what will be used here.\n",
    "\n",
    "The way it will be done is that the above library will provide a pool or workers, the number of which can be defined.  Each worker can then be asked to load a batch of data.  The batches returned will be returned as requested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "11f784fb",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from fastcore.basics import store_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2746c25",
   "metadata": {},
   "source": [
    "The way a dataset returns values based upon the index uses the getitem dunder as can be seen here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "b8907624",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 2, 1, 3]))"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__([[3,5,8,10]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "be044cb3",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]),\n",
       " tensor([1, 2, 1, 3]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds.__getitem__([3,5,8,10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe19996",
   "metadata": {},
   "source": [
    "It is possible to use the map function to split individual groups of items as below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "2fe54d75",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 2]))\n",
      "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([1, 3]))\n"
     ]
    }
   ],
   "source": [
    "for o in map(train_ds.__getitem__, [[3,5], [8,10]]):\n",
    "    print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed4396c",
   "metadata": {},
   "source": [
    "The dataloader can then be modified so that each worker in a pool loads a batch and return it when requested.  Note that it appears that the collate function is not needed here since the way in which the batch sampler passes a list of indecies results in an array of values being returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f9bfc6a3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, dataset, bs=64, shuffle=False, drop_last=False, num_workers=1):\n",
    "        fc.store_attr()\n",
    "        sampler = Sampler(ds=dataset, shuffle=shuffle)\n",
    "        self.batch_sampler = BatchSampler(sampler, bs, drop_last)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        with mp.Pool(self.num_workers) as ex:\n",
    "            yield from ex.map(self.dataset.__getitem__, iter(self.batch_sampler))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "8d647616",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, bs=64, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "3e321bfc",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "it = iter(train_dl)\n",
    "res = next(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9cf6603d",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([64, 784]), torch.Size([64]))"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb, yb = next(it)\n",
    "xb.shape, yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "cda81e55",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, torch.Size([64, 784]))"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(res),res[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d270d1c",
   "metadata": {},
   "source": [
    "### Pytorch DataLoader "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f5a0d1",
   "metadata": {},
   "source": [
    "Steps to follow:\n",
    "1. Create batch sampler\n",
    "2. create dataloader (with multi worker)\n",
    "3. Train model entirely using PyTorch\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fb7216b3",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_bs = BatchSampler(RandomSampler(train_ds), batch_size=bs, drop_last=False)\n",
    "valid_bs = BatchSampler(SequentialSampler(valid_ds), batch_size=bs, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "bf56c943",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# In this case the collate function is not required\n",
    "train_dl = DataLoader(train_ds, batch_sampler=train_bs, num_workers=4, collate_fn=collate)\n",
    "valid_dl = DataLoader(valid_ds, batch_sampler=valid_bs, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9235dd7a",
   "metadata": {},
   "source": [
    "In this case the collate function is not necessary as the dataset will already do this, as was shown above by the way a list of items returns stacked arrays.  Allowing Pytorch to autogenerate teh samplers as well then this can all be simplified to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "89879b1d",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, drop_last=True, num_workers=4)\n",
    "valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, drop_last=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4dc073",
   "metadata": {},
   "source": [
    "Check accuracy as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "845c7224",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After epoch 0, batch loss is: 0.3000752925872803\n",
      "After epoch 1, batch loss is: 0.034155331552028656\n",
      "After epoch 2, batch loss is: 0.07146771252155304\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0.12, grad_fn=<NllLossBackward0>), tensor(0.97))"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model,opt = get_model_and_optimizer()\n",
    "fit()\n",
    "\n",
    "loss_func(model(xb), yb), accuracy(model(xb), yb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa1a896",
   "metadata": {},
   "source": [
    "### Validation\n",
    "\n",
    "It is good (essential) practice to have a validation set and to check the accuracu of the model periodically, such as at the end of each epoch.\n",
    "\n",
    "Before calling the validation dataset it is necessary to put the model into eval mode, which avoids issues with batchnorm and dropout layers, where different setting are used for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b0926e",
   "metadata": {},
   "source": [
    "Create an update fit routine which calculates and prints out loss and accuracy at the end of each epoch.  As input define the number of epochs, the model to use, the loss function, the optimiser and the train and test dataloasers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L26){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fit\n",
       "\n",
       ">      fit (epochs, model, loss_func, opt, train_dl, valid_dl)"
      ],
      "text/plain": [
       "---\n",
       "\n",
       "[source](https://github.com/fastai/fastai-course-22p/blob/master/miniai/training.py#L26){target=\"_blank\" style=\"float:right; font-size:smaller\"}\n",
       "\n",
       "### fit\n",
       "\n",
       ">      fit (epochs, model, loss_func, opt, train_dl, valid_dl)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| echo: false\n",
    "#| output: asis\n",
    "show_doc(fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f86a720c",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "model,opt = get_model_and_optimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "70891f2f",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.16384194791316986, acc: 0.9517999887466431\n",
      "Epoch: 1, loss: 0.11577688157558441, acc: 0.963699996471405\n",
      "Epoch: 2, loss: 0.11353033781051636, acc: 0.9666000008583069\n"
     ]
    }
   ],
   "source": [
    "fit(epochs=3, model=model, loss_func=loss_func, opt=opt, train_dl=train_dl, valid_dl=valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe618c0",
   "metadata": {},
   "source": [
    "Finally create the dataloader using a function and then simplify the whole training process to three lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "9400f5f8",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_data_loaderers(train_ds, valid_ds, bs):\n",
    "    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\n",
    "    valid_ds = DataLoader(valid_ds, batch_size=bs, shuffle=False)\n",
    "    return train_dl, valid_dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c23179db",
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, loss: 0.20603491365909576, acc: 0.9343000054359436\n",
      "Epoch: 1, loss: 0.20518435537815094, acc: 0.9363999962806702\n",
      "Epoch: 2, loss: 0.1369418054819107, acc: 0.9598000049591064\n"
     ]
    }
   ],
   "source": [
    "model, opt = get_model_and_optimizer()\n",
    "train_dl, valid_dl = get_data_loaderers(train_ds, valid_ds, bs=bs)\n",
    "fit(epochs=3, model=model, loss_func=loss_func, opt=opt, train_dl=train_dl, valid_dl=valid_dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8cc6f1",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9895002b",
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
