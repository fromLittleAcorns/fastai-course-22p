# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/07_Convolutions.ipynb.

# %% auto 0
__all__ = ['def_device', 'conv', 'to_device', 'collate_device']

# %% ../nbs/07_Convolutions.ipynb 2
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl, numpy as np
import pandas as pd,matplotlib.pyplot as plt
from pathlib import Path
from torch import tensor
from torch import nn

from torch.utils.data import DataLoader,default_collate
from typing import Mapping

from .training import *
from .datasets import *

# %% ../nbs/07_Convolutions.ipynb 58
def conv(
    ni: int, # Input length
    nch: int, # number of channels output
    ks: int=3, # Kernel size (should be an odd number)
    stride: int=2, # Stride
    act: bool=True # Whether to assign an activation layer to the output
):
    out = nn.Conv2d(ni, nch, kernel_size=ks, stride=stride, padding=ks//2)
    if act: out = nn.Sequential(out, nn.ReLU())
    return out

# %% ../nbs/07_Convolutions.ipynb 71
# Set device according to the availability of gpus
#def_device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.backends.cuda.is_available() \
#    else "cpu"
def_device = "cpu" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() \
    else "cpu"

def to_device(x, device=def_device):
    # if x is a mapping then each of mapping targets needs to be moved to the device.  Needed to accomodate
    # huggingface examples.  Note also that the applications of type(x) prior to the (o.to(dev) for o in x)
    # results in the recreation of the tuple or list from the second part, which is a generator.
    if isinstance(x, torch.Tensor): return x.to(device)
    if isinstance(x, Mapping): return {k: v.to_device(device) for k, v in x.items()}
    return type(x)(o.to(device) for o in x)

def collate_device(b): return to_device(default_collate(b))
