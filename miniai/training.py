# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/04_dataloaders_optimisers_training.ipynb.

# %% auto 0
__all__ = ['accuracy', 'Dataset', 'fit', 'get_dls']

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 3
import pickle,gzip,math,os,time,shutil,torch,matplotlib as mpl,numpy as np,matplotlib.pyplot as plt
from pathlib import Path
from torch import tensor,nn
import torch.nn.functional as F

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 45
def accuracy(preds, targets):
    return (torch.argmax(preds, axis=1) == targets).float().mean()

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 96
class Dataset():
    def __init__(self, x, y): self.x = x; self.y = y
    def __len__(self): return len(self.x)
    def __getitem__(self, index): return self.x[index], self.y[index]

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 159
from torch.utils.data import DataLoader, SequentialSampler, RandomSampler, BatchSampler

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 169
def fit(epochs, model, loss_func, opt, train_dl, valid_dl):
    # Loop through the epochs
    for epoch in range(epochs):
        # Set model into training mode
        model.train()
        #iterate through each batch
        for xb, yb in train_dl:
            preds = model(xb)
            loss = loss_func(preds, yb)
            loss.backward()
            opt.step()
            opt.zero_grad()
        
        # Set model to eval mode for validation
        model.eval()
        # Run without grad calculations for validation (speed up model and use less memory)
        with torch.no_grad():
            # Reset the loss, accuracy and input count totals so that they can be used to sum values over the whole ds
            total_loss = 0.; total_acc=0.; total_count=0
            for xb, yb in valid_dl:
                n_items = len(yb)
                total_count += n_items
                preds = model(xb)
                total_loss += loss_func(preds, yb)*n_items
                total_acc += (torch.topk(preds, 1)[1][:,0]==yb).sum()
            print(f"Epoch: {epoch}, loss: {total_loss/total_count}, acc: {total_acc/total_count}")
    

# %% ../nbs/04_dataloaders_optimisers_training.ipynb 173
def get_dls(train_ds, valid_ds, bs, **kwargs):
    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True, **kwargs)
    valid_dl = DataLoader(valid_ds, batch_size=bs, shuffle=False, **kwargs)
    return train_dl, valid_dl
